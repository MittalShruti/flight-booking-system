{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CoNLL Chunking Corpus\n",
    "\n",
    "This notebook contains a standard process for building chunkers using the CoNLL chunking corpus from NLTK. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import conll2000\n",
    "import numpy as np\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading conll corpus in IO format\n",
    "train_iob = conll2000.iob_sents('train.txt')\n",
    "test_iob = conll2000.iob_sents('test.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8936\n"
     ]
    }
   ],
   "source": [
    "# there are 8936 sentences in the training data\n",
    "print(len(train_iob))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Confidence', 'NN', 'B-NP'),\n",
       " ('in', 'IN', 'B-PP'),\n",
       " ('the', 'DT', 'B-NP'),\n",
       " ('pound', 'NN', 'I-NP'),\n",
       " ('is', 'VBZ', 'B-VP'),\n",
       " ('widely', 'RB', 'I-VP'),\n",
       " ('expected', 'VBN', 'I-VP'),\n",
       " ('to', 'TO', 'I-VP'),\n",
       " ('take', 'VB', 'I-VP'),\n",
       " ('another', 'DT', 'B-NP'),\n",
       " ('sharp', 'JJ', 'I-NP'),\n",
       " ('dive', 'NN', 'I-NP'),\n",
       " ('if', 'IN', 'B-SBAR'),\n",
       " ('trade', 'NN', 'B-NP'),\n",
       " ('figures', 'NNS', 'I-NP'),\n",
       " ('for', 'IN', 'B-PP'),\n",
       " ('September', 'NNP', 'B-NP'),\n",
       " (',', ',', 'O'),\n",
       " ('due', 'JJ', 'B-ADJP'),\n",
       " ('for', 'IN', 'B-PP'),\n",
       " ('release', 'NN', 'B-NP'),\n",
       " ('tomorrow', 'NN', 'B-NP'),\n",
       " (',', ',', 'O'),\n",
       " ('fail', 'VB', 'B-VP'),\n",
       " ('to', 'TO', 'I-VP'),\n",
       " ('show', 'VB', 'I-VP'),\n",
       " ('a', 'DT', 'B-NP'),\n",
       " ('substantial', 'JJ', 'I-NP'),\n",
       " ('improvement', 'NN', 'I-NP'),\n",
       " ('from', 'IN', 'B-PP'),\n",
       " ('July', 'NNP', 'B-NP'),\n",
       " ('and', 'CC', 'I-NP'),\n",
       " ('August', 'NNP', 'I-NP'),\n",
       " (\"'s\", 'POS', 'B-NP'),\n",
       " ('near-record', 'JJ', 'I-NP'),\n",
       " ('deficits', 'NNS', 'I-NP'),\n",
       " ('.', '.', 'O')]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sample IOB tagged sentence\n",
    "train_iob[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distribution of Chunks\n",
    "\n",
    "Let's have a quick look at the distribution of chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store all chunk tags in a list of chunks\n",
    "train_chunk_tags = [tup[2] for sent in train_iob for tup in sent]\n",
    "test_chunk_tags = [tup[2] for sent in test_iob for tup in sent]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22\n",
      "{'B-ADJP', 'I-PRT', 'I-ADJP', 'B-NP', 'I-ADVP', 'I-CONJP', 'I-NP', 'B-VP', 'I-SBAR', 'I-UCP', 'B-UCP', 'B-ADVP', 'B-PRT', 'I-VP', 'O', 'I-INTJ', 'B-SBAR', 'B-CONJP', 'B-LST', 'I-PP', 'B-INTJ', 'B-PP'}\n"
     ]
    }
   ],
   "source": [
    "#there are total 22 chunk tags (IOB format)\n",
    "chunk_set = set(chunk_tags)\n",
    "print(len(chunk_set))\n",
    "print(chunk_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conversion from IOB to Tree Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (NP Confidence/NN)\n",
      "  (PP in/IN)\n",
      "  (NP the/DT pound/NN)\n",
      "  (VP is/VBZ widely/RB expected/VBN to/TO take/VB)\n",
      "  (NP another/DT sharp/JJ dive/NN)\n",
      "  (SBAR if/IN)\n",
      "  (NP trade/NN figures/NNS)\n",
      "  (PP for/IN)\n",
      "  (NP September/NNP)\n",
      "  ,/,\n",
      "  (ADJP due/JJ)\n",
      "  (PP for/IN)\n",
      "  (NP release/NN)\n",
      "  (NP tomorrow/NN)\n",
      "  ,/,\n",
      "  (VP fail/VB to/TO show/VB)\n",
      "  (NP a/DT substantial/JJ improvement/NN)\n",
      "  (PP from/IN)\n",
      "  (NP July/NNP and/CC August/NNP)\n",
      "  (NP 's/POS near-record/JJ deficits/NNS)\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "# use the conlltags2tree method\n",
    "print(nltk.chunk.conlltags2tree(train_iob[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tree Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# can read the sentences directly in Tree format\n",
    "train_sents = conll2000.chunked_sents('train.txt')\n",
    "test_sents = conll2000.chunked_sents('test.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nltk.tree.Tree"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# each training sentence is of type nltk.tree\n",
    "type(train_sents[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (NP Confidence/NN)\n",
      "  (PP in/IN)\n",
      "  (NP the/DT pound/NN)\n",
      "  (VP is/VBZ widely/RB expected/VBN to/TO take/VB)\n",
      "  (NP another/DT sharp/JJ dive/NN)\n",
      "  if/IN\n",
      "  (NP trade/NN figures/NNS)\n",
      "  (PP for/IN)\n",
      "  (NP September/NNP)\n",
      "  ,/,\n",
      "  due/JJ\n",
      "  (PP for/IN)\n",
      "  (NP release/NN)\n",
      "  (NP tomorrow/NN)\n",
      "  ,/,\n",
      "  (VP fail/VB to/TO show/VB)\n",
      "  (NP a/DT substantial/JJ improvement/NN)\n",
      "  (PP from/IN)\n",
      "  (NP July/NNP and/CC August/NNP)\n",
      "  (NP 's/POS near-record/JJ deficits/NNS)\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "# sample chunked sentence\n",
    "# has three labels for chunks - NP chunk, VP chunk, PP chunk\n",
    "print(train_sents[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Confidence', 'NN', 'B-NP'),\n",
       " ('in', 'IN', 'B-PP'),\n",
       " ('the', 'DT', 'B-NP'),\n",
       " ('pound', 'NN', 'I-NP'),\n",
       " ('is', 'VBZ', 'B-VP'),\n",
       " ('widely', 'RB', 'I-VP'),\n",
       " ('expected', 'VBN', 'I-VP'),\n",
       " ('to', 'TO', 'I-VP'),\n",
       " ('take', 'VB', 'I-VP'),\n",
       " ('another', 'DT', 'B-NP'),\n",
       " ('sharp', 'JJ', 'I-NP'),\n",
       " ('dive', 'NN', 'I-NP'),\n",
       " ('if', 'IN', 'B-SBAR'),\n",
       " ('trade', 'NN', 'B-NP'),\n",
       " ('figures', 'NNS', 'I-NP'),\n",
       " ('for', 'IN', 'B-PP'),\n",
       " ('September', 'NNP', 'B-NP'),\n",
       " (',', ',', 'O'),\n",
       " ('due', 'JJ', 'B-ADJP'),\n",
       " ('for', 'IN', 'B-PP'),\n",
       " ('release', 'NN', 'B-NP'),\n",
       " ('tomorrow', 'NN', 'B-NP'),\n",
       " (',', ',', 'O'),\n",
       " ('fail', 'VB', 'B-VP'),\n",
       " ('to', 'TO', 'I-VP'),\n",
       " ('show', 'VB', 'I-VP'),\n",
       " ('a', 'DT', 'B-NP'),\n",
       " ('substantial', 'JJ', 'I-NP'),\n",
       " ('improvement', 'NN', 'I-NP'),\n",
       " ('from', 'IN', 'B-PP'),\n",
       " ('July', 'NNP', 'B-NP'),\n",
       " ('and', 'CC', 'I-NP'),\n",
       " ('August', 'NNP', 'I-NP'),\n",
       " (\"'s\", 'POS', 'B-NP'),\n",
       " ('near-record', 'JJ', 'I-NP'),\n",
       " ('deficits', 'NNS', 'I-NP'),\n",
       " ('.', '.', 'O')]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the corresponding IOB format is this:\n",
    "train_iob[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conversion from Tree to IOB Format\n",
    "\n",
    "The following method can convert from tree to IOB format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Confidence', 'NN', 'B-NP'),\n",
       " ('in', 'IN', 'B-PP'),\n",
       " ('the', 'DT', 'B-NP'),\n",
       " ('pound', 'NN', 'I-NP'),\n",
       " ('is', 'VBZ', 'B-VP'),\n",
       " ('widely', 'RB', 'I-VP'),\n",
       " ('expected', 'VBN', 'I-VP'),\n",
       " ('to', 'TO', 'I-VP'),\n",
       " ('take', 'VB', 'I-VP'),\n",
       " ('another', 'DT', 'B-NP'),\n",
       " ('sharp', 'JJ', 'I-NP'),\n",
       " ('dive', 'NN', 'I-NP'),\n",
       " ('if', 'IN', 'O'),\n",
       " ('trade', 'NN', 'B-NP'),\n",
       " ('figures', 'NNS', 'I-NP'),\n",
       " ('for', 'IN', 'B-PP'),\n",
       " ('September', 'NNP', 'B-NP'),\n",
       " (',', ',', 'O'),\n",
       " ('due', 'JJ', 'O'),\n",
       " ('for', 'IN', 'B-PP'),\n",
       " ('release', 'NN', 'B-NP'),\n",
       " ('tomorrow', 'NN', 'B-NP'),\n",
       " (',', ',', 'O'),\n",
       " ('fail', 'VB', 'B-VP'),\n",
       " ('to', 'TO', 'I-VP'),\n",
       " ('show', 'VB', 'I-VP'),\n",
       " ('a', 'DT', 'B-NP'),\n",
       " ('substantial', 'JJ', 'I-NP'),\n",
       " ('improvement', 'NN', 'I-NP'),\n",
       " ('from', 'IN', 'B-PP'),\n",
       " ('July', 'NNP', 'B-NP'),\n",
       " ('and', 'CC', 'I-NP'),\n",
       " ('August', 'NNP', 'I-NP'),\n",
       " (\"'s\", 'POS', 'B-NP'),\n",
       " ('near-record', 'JJ', 'I-NP'),\n",
       " ('deficits', 'NNS', 'I-NP'),\n",
       " ('.', '.', 'O')]"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.chunk.tree2conlltags(train_sents[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Chunkers\n",
    "\n",
    "### Dummy Parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dummy parser: tag everything as 'O'\n",
    "cp = nltk.RegexpParser('')\n",
    "test_sents = conll2000.chunked_sents('test.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (NP Rockwell/NNP International/NNP Corp./NNP)\n",
      "  (NP 's/POS Tulsa/NNP unit/NN)\n",
      "  (VP said/VBD)\n",
      "  (NP it/PRP)\n",
      "  (VP signed/VBD)\n",
      "  (NP a/DT tentative/JJ agreement/NN)\n",
      "  (VP extending/VBG)\n",
      "  (NP its/PRP$ contract/NN)\n",
      "  (PP with/IN)\n",
      "  (NP Boeing/NNP Co./NNP)\n",
      "  (VP to/TO provide/VB)\n",
      "  (NP structural/JJ parts/NNS)\n",
      "  (PP for/IN)\n",
      "  (NP Boeing/NNP)\n",
      "  (NP 's/POS 747/CD jetliners/NNS)\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "# sample test sentence\n",
    "print(test_sents[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse the test sentences\n",
    "test_parsed = cp.parse(test_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChunkParse score:\n",
      "    IOB Accuracy:  17.8%%\n",
      "    Precision:      0.0%%\n",
      "    Recall:         0.0%%\n",
      "    F-Measure:      0.0%%\n"
     ]
    }
   ],
   "source": [
    "# evaluate the parsed sentences\n",
    "print(cp.evaluate(test_sents))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An accuracy of 17.8% indicates that about as many words are tagged with a 'O', i.e. belonging to none of the three chunks. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChunkParse score:\n",
      "    IOB Accuracy:  62.5%%\n",
      "    Precision:     70.6%%\n",
      "    Recall:        38.5%%\n",
      "    F-Measure:     49.8%%\n"
     ]
    }
   ],
   "source": [
    "# NP parser: any POS tag beginning with C, D, J, P is to be tagged as a NP chunk \n",
    "cp = nltk.RegexpParser(r'''\n",
    "NP: {<[CDJNP].*>+}\n",
    "''')\n",
    "\n",
    "# evaluate\n",
    "print(cp.evaluate(test_sents))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also add verb phrases (again using a naive logic - any sequence of POS tags that contains one or more words that start with a V are to be tagged as a VP chunk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChunkParse score:\n",
      "    IOB Accuracy:  71.6%%\n",
      "    Precision:     67.1%%\n",
      "    Recall:        53.2%%\n",
      "    F-Measure:     59.3%%\n"
     ]
    }
   ],
   "source": [
    "# NP and VP parser\n",
    "cp = nltk.RegexpParser(r'''\n",
    "NP: {<[CDJNP].*>+} \n",
    "VP: {<[V].*>+}''')\n",
    "\n",
    "# evaluate\n",
    "print(cp.evaluate(test_sents))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unigram Chunker\n",
    "\n",
    "Let's now try a simple unigram chunker - given a part of speech tag, it will identify the most likely chunk tag (NP, VP or PP chunk) for a given word (using the word's POS tag). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unigram tagger\n",
    "\n",
    "from nltk import ChunkParserI\n",
    "\n",
    "class UnigramChunker(ChunkParserI):    \n",
    "    def __init__(self, train_sents):\n",
    "        # convert train sents from tree format to tags\n",
    "        train_data = [[(t, c) for w, t, c in nltk.chunk.tree2conlltags(sent)] \n",
    "                      for sent in train_sents]\n",
    "        self.tagger = nltk.UnigramTagger(train_data)\n",
    "        \n",
    "    def parse(self, sentence):\n",
    "        pos_tags = [pos for (word, pos) in sentence]\n",
    "        tagged_pos_tags = self.tagger.tag(pos_tags)\n",
    "        chunktags = [chunktag for (pos, chunktag) in tagged_pos_tags]\n",
    "        \n",
    "        # convert to tree again\n",
    "        conlltags = [(word, pos, chunktag) for ((word, pos), chunktag) in zip(sentence, chunktags)]\n",
    "        return nltk.chunk.conlltags2tree(conlltags)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use the UnigramChunker to tag the conll sentences. Note that the sentences have to be passed in the tree format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChunkParse score:\n",
      "    IOB Accuracy:  86.5%%\n",
      "    Precision:     74.3%%\n",
      "    Recall:        86.4%%\n",
      "    F-Measure:     79.9%%\n"
     ]
    }
   ],
   "source": [
    "# read the sentences in tree format\n",
    "train_sents = conll2000.chunked_sents('train.txt')\n",
    "test_sents = conll2000.chunked_sents('test.txt')\n",
    "\n",
    "# unigram chunker \n",
    "unigram_chunker = UnigramChunker(train_sents)\n",
    "print(unigram_chunker.evaluate(test_sents))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The unigram chunker seems to be doing much better than the previous ones. Rest of the chunkers are applied on the ATIS dataset."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
