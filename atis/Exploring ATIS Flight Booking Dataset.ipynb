{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ATIS Flight Reservations Dataset\n",
    "\n",
    "Dataset download link: http://lisaweb.iro.umontreal.ca/transfert/lisa/users/mesnilgr/atis/\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk, pprint, os\n",
    "import gzip, os, pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# read the first part of the dataset\n",
    "# each part (.gz file) contains train, validation and test sets, plus a dict\n",
    "\n",
    "filename = 'atis.fold0.pkl.gz'\n",
    "f = gzip.open(filename, 'rb')\n",
    "try:\n",
    "    train_set, valid_set, test_set, dicts = pickle.load(f, encoding='latin1')\n",
    "except:\n",
    "    train_set, valid_set, test_set, dicts = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 3983)\n",
      "(3, 995)\n",
      "(3, 893)\n"
     ]
    }
   ],
   "source": [
    "# structure of the component data files\n",
    "print(np.shape(train_set))\n",
    "print(np.shape(valid_set))\n",
    "print(np.shape(test_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "<class 'list'>\n",
      "3983\n"
     ]
    }
   ],
   "source": [
    "# each set is a 3-tuple, each element of the tuple being a list \n",
    "print(len(train_set))\n",
    "print(type(train_set[0]))\n",
    "print(len(train_set[0]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first list has 3983 arrays, each array being a sentence. The words are encoded by numbers (and have to be decoded using the dict provided).\n",
    "\n",
    "Let's store the three lists into separate objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "# storing the three elements of the tuple in three objects\n",
    "train_x, _, train_label = train_set\n",
    "val_x, _, val_label = valid_set\n",
    "test_x, _, test_label = test_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first list represents the actual words (encoded), and the third list contains their labels (again, encoded)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([554,  23, 241, 534, 358, 136, 193,  11, 208, 251, 104, 502, 413,\n",
       "       256, 104])"
      ]
     },
     "execution_count": 295,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# each list in the tuple is a numpy array (a sentence)\n",
    "# printing first list in the tuple's first element\n",
    "train_x[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([126, 126, 126, 126, 126,   2, 126,  43, 126,  48, 109, 126,  78,\n",
       "       123, 123])"
      ]
     },
     "execution_count": 296,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# labels are stored in the third list train_label\n",
    "train_label[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n",
      "dict_keys(['labels2idx', 'tables2idx', 'words2idx'])\n"
     ]
    }
   ],
   "source": [
    "# dicts \n",
    "print(type(dicts))\n",
    "print(dicts.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n"
     ]
    }
   ],
   "source": [
    "# each key:value pair is itself a dict\n",
    "print(type(dicts['labels2idx']))\n",
    "print(type(dicts['tables2idx']))\n",
    "print(type(dicts['words2idx']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [],
   "source": [
    "# storing labels and words in separate variables\n",
    "words = dicts['words2idx']\n",
    "labels = dicts['labels2idx']\n",
    "tables = dicts['tables2idx']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['all', 'coach', 'cincinnati', 'people', 'month', 'four', 'code', 'go', 'show', 'thursday', 'to', 'restriction', 'dinnertime', 'under', 'sorry', 'include', 'midwest', 'worth', 'southwest', 'me', 'returning', 'far', 'vegas', 'airfare', 'ticket', 'difference', 'arrange', 'tickets', 'louis', 'cheapest', 'list', 'wednesday', 'leave', 'heading', 'ten', 'direct', 'turboprop', 'rate', 'cost', 'quebec', 'layover', 'air', 'what', 'stands', 'chicago', 'schedule', 'transcontinental', 'goes', 'new', 'transportation', 'here', 'hours', 'let', 'twentieth', 'along', 'thrift', 'passengers', 'great', 'thirty', 'canadian', 'leaves', 'alaska', 'leaving', 'amount', 'weekday', 'makes', 'midway', 'montreal', 'via', 'depart', 'county', 'names', 'stand', 'total', 'seventeenth', 'use', 'twa', 'from', 'would', 'abbreviations', 'destination', 'only', 'next', 'live', 'shortest', 'limousine', 'tell', 'today', 'more', 'DIGIT', 'm80', 'downtown', 'train', 'tampa', 'fly', 'f', 'this', 'car', 'anywhere', 'can', 'following', 'making', 'arrive', 'my', 'could', 'give', 'december', 'numbers', 'want', 'DIGITDIGITDIGITDIGITDIGITDIGIT', 'airplane', 'times', 'information', 'tacoma', 'provide', 'travel', 'six', 'dinner', 'located', 'sunday', 'fourth', 'types', 'beach', 'nonstop', 'economy', 'fare', 'okay', 'y', 'may', 'earlier', 'plane', 'ff', 'coming', 'eighth', 'fn', 'las', 'a', 'boeing', 'third', 'departure', 'q', 'so', 'rentals', 'houston', 'serving', 'help', 'september', 'over', 'midnight', 'soon', 'logan', 'through', 'milwaukee', 'still', 'before', 'thirtieth', 'thank', 'fit', 'how', 'trying', 'denver', 'actually', 'late', 'offers', 'listing', 'texas', 'DIGITDIGITDIGITDIGIT', 'then', 'evening', 'return', 'yn', 'lunch', 'wednesdays', 'they', 'arriving', 'now', 'rental', 'day', 'landings', 'february', 'airports', 'name', 'sundays', 'january', 'detroit', 'each', 'meal', 'dulles', 'petersburg', 'thirteenth', 'ea', 'used', 'arrives', 'connect', 'requesting', 'tenth', 'saturday', 'out', 'canada', 'looking', 'arizona', 'cars', 'friday', 'seventh', 'california', 'york', 'bwi', 'ord', 'earliest', 'repeat', 'dc10', 'atl', 'florida', 'days', 'round', 'american', 'afternoon', 'st.', 'first', 'there', 'number', 'one', 'eleventh', 'approximately', 'another', 'tomorrow', '<UNK>', 'city', 'service', 'twenty', 'dfw', 'weekdays', 'least', 'their', 'rates', 'DIGITDIGITDIGIT', 'time', 'too', 'sixteenth', 'that', 'pittsburgh', 'serve', 'july', 'than', 'toronto', 'distance', 'kind', 'b', 'second', 'pennsylvania', 'classes', 'other', 'traveling', 'and', 'charlotte', 'san', 'stopovers', 'boston', 'takeoff', 'say', 'buy', 'rent', 'have', 'need', 'breakfast', 'philly', 'any', 'sa', 'dallas', 'also', 'without', 'take', 'which', 'sure', 'price', 'who', 'serviced', 'most', 'eight', 'landing', 'services', 'america', 'class', 'later', 'm', 'nineteenth', 'salt', 'departing', 'cheap', 'tuesdays', 'find', 'fifth', 'ground', 'snack', 'with', 'explain', 'minnesota', 'should', 'flights', 'going', 'qx', 'carolina', 'do', 'dl', 'get', 'michigan', 'express', 'stop', 'dc', 'international', 'during', 'westchester', 'qw', 'stapleton', 'h', 'morning', 'wish', 'ohio', 'where', 'qo', 'arrival', 'eighteenth', 'up', 'connections', 'see', 'are', 'close', 'yes', 'capacity', 'please', 'smallest', 'various', 'between', 'f28', 'available', 'we', 'august', 's', 'nashville', 'aircraft', 'fifteenth', 'cities', 'jfk', 'both', 'c', 'last', 'many', 'taking', 'la', 'trips', 'april', 'connection', 'baltimore', 'flies', 'co', 'tuesday', 'nonstops', 'tennessee', 'stopover', 'cp', 'november', 'expensive', 'west', 'airlines', 'nationair', 'much', 'define', 'mco', 'flight', 'eastern', 'airplanes', 'lives', 'prices', 'atlanta', 'an', 'those', 'sfo', 'georgia', 'look', 'these', 'originate', 'choices', 'will', 'near', 'itinerary', 'stopping', 'mitchell', 'fourteenth', 'thursdays', 'is', 'it', 'arrangements', 'in', 'seattle', 'if', 'different', 'make', 'airport', 'same', 'northwest', 'ewr', 'twelfth', 'week', 'indianapolis', 'diego', 'takeoffs', 'uses', 'two', 'database', 'i', 'well', 'options', 'costs', 'jersey', 'very', 'the', 'latest', 'taxi', 'just', 'less', 'ninth', 'abbreviation', 'seats', 'love', 'paul', 'jose', 'sixteen', 'lake', 'book', 'ap80', 'fares', 'has', 'march', 'around', 'utah', 'possible', 'early', 'know', 'schedules', 'using', 'like', 'd', 'miami', 'orlando', 'arrivals', 'either', 'night', 'served', 'tower', 'limo', 'seating', 'right', 'saturdays', 'lastest', 'some', 'back', \"'t\", 'serves', 'cleveland', \"'s\", 'transport', 'provided', 'oakland', 'phoenix', 'for', 'noon', 'stops', 'newark', 'does', 'connecting', 'booking', 'be', 'columbus', 'business', 'reaching', 'sixth', 'departures', 'ap57', 'by', 'after', 'on', 'about', 'noontime', 'DIGITDIGIT', 'of', 'dollars', 'burbank', 'angeles', 'carries', 'airline', 'mean', 'or', 'plan', 'colorado', 'united', 'into', 'within', 'washington', 'bound', 'three', 'your', 'guardia', 'ontario', 'area', 'flying', 'philadelphia', 'long', 'continental', 'los', '72s', 'way', 'lowest', 'mornings', 'north', 'offer', 'hp', 'restrictions', 'but', 'hi', 'delta', 'highest', 'memphis', 'fort', 'october', 'type', 'maximum', 'us', \"'re\", 'planes', 'pm', 'ua', 'display', 'originating', 'ac', 'reservations', 'describe', 'am', 'minneapolis', 'general', 'ap', 'as', 'sometime', 'at', 'trip', 'again', 'codes', \"'ll\", 'no', 'when', 'field', 'interested', 'you', 'nw', 'francisco', 'kinds', 'monday', \"o'clock\", 'kansas', 'june', 'lufthansa', 'meaning', \"'d\", 'reservation', \"'m\", 'friends', 'meals', 'land', 'daily', 'departs', 'missouri', 'starting', 'hello'])"
      ]
     },
     "execution_count": 300,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# each key of words_dict is a word, each value its index\n",
    "words.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['what',\n",
       " 'aircraft',\n",
       " 'is',\n",
       " 'used',\n",
       " 'on',\n",
       " 'delta',\n",
       " 'flight',\n",
       " 'DIGITDIGITDIGITDIGIT',\n",
       " 'from',\n",
       " 'kansas',\n",
       " 'city',\n",
       " 'to',\n",
       " 'salt',\n",
       " 'lake',\n",
       " 'city']"
      ]
     },
     "execution_count": 301,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now, we can map the numeric values v in a sentence with the k,v in the dict\n",
    "# train_x contains the list of training sentences\n",
    "# this is the first sentence\n",
    "[k for val in train_x[0] for k,v in words.items() if v==val]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['what aircraft is used on delta flight DIGITDIGITDIGITDIGIT from kansas city to salt lake city',\n",
       " 'i want to go from boston to atlanta on monday',\n",
       " \"i need a flight from atlanta to philadelphia and i 'm looking for the cheapest fare\",\n",
       " 'i need a flight from toronto to montreal reaching montreal early on friday',\n",
       " 'show me the evening flights from philadelphia to baltimore',\n",
       " 'tell me distance from orlando airport to the city',\n",
       " 'what is restriction ap80',\n",
       " 'what is the lowest cost fare that delta has between boston and san francisco',\n",
       " 'flight DIGITDIGITDIGIT from cincinnati to dallas',\n",
       " 'now i need a one way flight from pittsburgh to denver',\n",
       " 'display all flights leaving from toronto to san diego on us air <UNK> over in washington dc',\n",
       " 'list all nonstop flights on tuesday before noon from charlotte to baltimore',\n",
       " 'show me the lowest <UNK> fare from dallas to baltimore',\n",
       " 'what is the cheapest coach flight between dallas and baltimore leaving august tenth',\n",
       " 'does midwest express serve philadelphia',\n",
       " 'what flights on united leave la guardia for san jose and arrive around DIGITDIGIT pm',\n",
       " 'list the nonstop flights from denver to washington dc',\n",
       " 'how much does flight ua DIGITDIGITDIGIT from denver to san francisco cost',\n",
       " 'what kind of ground transportation is available in las vegas',\n",
       " 'what are the nonstop flights from kansas city to burbank arriving on saturday may twenty two',\n",
       " 'list the morning flights at a DIGITDIGITDIGIT dollars from atlanta to boston',\n",
       " 'what is the fare going from atlanta to boston one way on november seventh',\n",
       " 'is there a flight from atlanta to san francisco that stops over in denver',\n",
       " \"find travel arrangements for a round trip flight from baltimore to pittsburgh after DIGIT o'clock pm before DIGITDIGIT o'clock pm\",\n",
       " 'can i have a list of all the thursday flights from baltimore to atlanta that leave after DIGIT pm',\n",
       " 'what flights do you have in the morning of september twentieth on united airlines from pittsburgh to san francisco and a stopover in denver',\n",
       " 'i need a flight from boston to pittsburgh that leaves early in the morning',\n",
       " 'round trip houston to las vegas nonstop',\n",
       " 'flight information from san francisco to pittsburgh',\n",
       " 'give me flights without fares from atlanta to baltimore']"
      ]
     },
     "execution_count": 302,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's look at the first few sentences\n",
    "sents = []\n",
    "for i in range(30):\n",
    "    sents.append(' '.join([k for val in train_x[i] for k,v in words.items() if v==val]))\n",
    "\n",
    "sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['B-time_relative', 'B-stoploc.state_code', 'B-depart_date.today_relative', 'B-arrive_date.date_relative', 'B-depart_date.date_relative', 'I-restriction_code', 'B-return_date.month_name', 'I-time', 'B-depart_date.day_name', 'I-arrive_time.end_time', 'B-fromloc.airport_code', 'B-cost_relative', 'B-connect', 'B-return_time.period_mod', 'B-arrive_time.period_mod', 'B-flight_number', 'B-depart_time.time_relative', 'I-toloc.city_name', 'B-arrive_time.period_of_day', 'B-depart_time.period_of_day', 'I-return_date.date_relative', 'I-depart_time.start_time', 'B-fare_amount', 'I-depart_time.time_relative', 'B-city_name', 'B-depart_date.day_number', 'I-meal_description', 'I-depart_date.today_relative', 'I-airport_name', 'I-arrive_date.day_number', 'B-toloc.state_code', 'B-arrive_date.month_name', 'B-stoploc.airport_code', 'I-depart_time.time', 'B-airport_code', 'B-arrive_time.start_time', 'B-period_of_day', 'B-arrive_time.time', 'I-flight_stop', 'B-toloc.state_name', 'B-booking_class', 'I-meal_code', 'B-arrive_time.end_time', 'B-meal', 'B-arrive_time.time_relative', 'B-return_date.day_number', 'I-city_name', 'B-day_name', 'B-or', 'I-flight_mod', 'I-arrive_time.time', 'B-economy', 'B-fromloc.airport_name', 'B-return_date.day_name', 'O', 'B-class_type', 'B-meal_code', 'B-depart_time.time', 'B-return_date.today_relative', 'B-round_trip', 'B-restriction_code', 'B-fare_basis_code', 'I-stoploc.city_name', 'I-fare_basis_code', 'B-flight', 'I-fromloc.airport_name', 'B-compartment', 'B-airline_code', 'B-fromloc.state_name', 'B-flight_stop', 'B-day_number', 'B-flight_mod', 'I-arrive_time.period_of_day', 'B-depart_time.start_time', 'B-today_relative', 'I-arrive_time.time_relative', 'B-arrive_date.day_number', 'I-flight_time', 'B-arrive_date.day_name', 'I-fromloc.state_name', 'B-mod', 'B-depart_date.month_name', 'B-flight_days', 'I-cost_relative', 'B-stoploc.airport_name', 'I-today_relative', 'B-fromloc.city_name', 'B-transport_type', 'B-return_time.period_of_day', 'B-time', 'B-toloc.country_name', 'B-return_date.date_relative', 'I-depart_date.day_number', 'I-transport_type', 'I-fromloc.city_name', 'B-depart_date.year', 'I-return_date.day_number', 'B-flight_time', 'B-toloc.city_name', 'B-depart_time.period_mod', 'I-arrive_time.start_time', 'B-state_code', 'B-airport_name', 'B-stoploc.city_name', 'I-toloc.airport_name', 'B-meal_description', 'I-class_type', 'B-toloc.airport_code', 'I-depart_time.period_of_day', 'I-toloc.state_name', 'B-days_code', 'B-toloc.airport_name', 'B-arrive_date.today_relative', 'I-round_trip', 'I-state_name', 'I-fare_amount', 'B-airline_name', 'I-flight_number', 'I-airline_name', 'B-state_name', 'I-economy', 'B-depart_time.end_time', 'B-aircraft_code', 'I-return_date.today_relative', 'B-month_name', 'B-fromloc.state_code', 'I-depart_time.end_time'])"
      ]
     },
     "execution_count": 303,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# labels dict contains IOB (inside-out-beginning) labelled entities\n",
    "labels.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 127 classes of labels (including the 'O' - tokens that do not fall into any entity)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "127\n"
     ]
    }
   ],
   "source": [
    "# number of labels\n",
    "print(len(labels.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the dicts 'words' and 'labels' are key:value pairs of index:word/label, let's reverse the dicts so that we don't have to do a reverse lookup everytime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting words_to_id to id_to_words\n",
    "# and labels_to_id to id_to_labels\n",
    "id_to_words = {words[k]:k for k in words}\n",
    "id_to_labels = {labels[k]:k for k in labels}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can print the words and corresponding labels simply by looking up the value of a numeric index of each word, for e.g.:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('tell', 'O'), ('me', 'O'), ('about', 'O'), ('twa', 'B-airline_code'), ('flight', 'O'), ('DIGITDIGITDIGIT', 'B-flight_number')]\n",
      "\n",
      "\n",
      "[('i', 'O'), (\"'d\", 'O'), ('like', 'O'), ('to', 'O'), ('fly', 'O'), ('from', 'O'), ('denver', 'B-fromloc.city_name'), ('to', 'O'), ('atlanta', 'B-toloc.city_name'), ('with', 'O'), ('a', 'O'), ('stop', 'O'), ('in', 'O'), ('pittsburgh', 'B-stoploc.city_name')]\n",
      "\n",
      "\n",
      "[('what', 'O'), ('is', 'O'), ('restriction', 'O'), ('ap57', 'B-restriction_code')]\n",
      "\n",
      "\n",
      "[('show', 'O'), ('me', 'O'), ('all', 'O'), ('flights', 'O'), ('from', 'O'), ('montreal', 'B-fromloc.city_name'), ('to', 'O'), ('nashville', 'B-toloc.city_name')]\n",
      "\n",
      "\n",
      "[('how', 'O'), ('do', 'O'), ('you', 'O'), ('get', 'O'), ('from', 'O'), ('the', 'O'), ('airport', 'O'), ('to', 'O'), ('downtown', 'O'), ('dallas', 'B-toloc.city_name'), ('please', 'O')]\n",
      "\n",
      "\n",
      "[('i', 'O'), ('would', 'O'), ('like', 'O'), ('to', 'O'), ('fly', 'O'), ('from', 'O'), ('dallas', 'B-fromloc.city_name'), ('to', 'O'), ('san', 'B-toloc.city_name'), ('francisco', 'I-toloc.city_name'), ('on', 'O'), ('saturday', 'B-depart_date.day_name'), ('and', 'O'), ('arrive', 'O'), ('in', 'O'), ('san', 'B-toloc.city_name'), ('francisco', 'I-toloc.city_name'), ('before', 'B-arrive_time.time_relative'), ('DIGIT', 'B-arrive_time.time'), ('pm', 'I-arrive_time.time')]\n",
      "\n",
      "\n",
      "[('what', 'O'), ('is', 'O'), ('airline', 'O'), ('us', 'B-airline_code')]\n",
      "\n",
      "\n",
      "[('describe', 'O'), ('ground', 'O'), ('transportation', 'O'), ('in', 'O'), ('dallas', 'B-city_name')]\n",
      "\n",
      "\n",
      "[('show', 'O'), ('me', 'O'), ('the', 'O'), ('flights', 'O'), ('from', 'O'), ('boston', 'B-fromloc.city_name'), ('to', 'O'), ('oakland', 'B-toloc.city_name')]\n",
      "\n",
      "\n",
      "[('flights', 'O'), ('from', 'O'), ('miami', 'B-fromloc.city_name'), ('to', 'O'), ('cleveland', 'B-toloc.city_name')]\n",
      "\n",
      "\n",
      "[('and', 'O'), ('flight', 'O'), ('from', 'O'), ('oakland', 'B-fromloc.city_name'), ('to', 'O'), ('boston', 'B-toloc.city_name'), ('leaving', 'O'), ('after', 'B-depart_time.time_relative'), ('midnight', 'B-depart_time.period_of_day')]\n",
      "\n",
      "\n",
      "[('show', 'O'), ('me', 'O'), ('flights', 'O'), ('between', 'O'), ('milwaukee', 'B-fromloc.city_name'), ('and', 'O'), ('phoenix', 'B-toloc.city_name'), ('on', 'O'), ('saturday', 'B-depart_date.day_name'), ('or', 'B-or'), ('sunday', 'B-depart_date.day_name'), ('american', 'B-airline_name'), ('airlines', 'I-airline_name')]\n",
      "\n",
      "\n",
      "[('what', 'O'), ('flights', 'O'), ('are', 'O'), ('there', 'O'), ('from', 'O'), ('baltimore', 'B-fromloc.city_name'), ('to', 'O'), ('newark', 'B-toloc.city_name')]\n",
      "\n",
      "\n",
      "[('how', 'O'), ('many', 'O'), ('airlines', 'O'), ('have', 'O'), ('flights', 'O'), ('with', 'O'), ('service', 'O'), ('class', 'O'), ('yn', 'B-fare_basis_code')]\n",
      "\n",
      "\n",
      "[('i', 'O'), ('want', 'O'), ('a', 'O'), ('flight', 'O'), ('from', 'O'), ('houston', 'B-fromloc.city_name'), ('to', 'O'), ('memphis', 'B-toloc.city_name'), ('on', 'O'), ('tuesday', 'B-depart_date.day_name'), ('morning', 'B-depart_time.period_of_day')]\n",
      "\n",
      "\n",
      "[('give', 'O'), ('me', 'O'), ('the', 'O'), ('flights', 'O'), ('from', 'O'), ('indianapolis', 'B-fromloc.city_name'), ('to', 'O'), ('orlando', 'B-toloc.city_name'), ('on', 'O'), ('december', 'B-depart_date.month_name'), ('twenty', 'B-depart_date.day_number'), ('seventh', 'I-depart_date.day_number'), ('and', 'O'), ('twenty', 'B-depart_date.day_number'), ('eighth', 'I-depart_date.day_number')]\n",
      "\n",
      "\n",
      "[('what', 'O'), (\"'s\", 'O'), ('the', 'O'), ('ground', 'O'), ('transportation', 'O'), ('like', 'O'), ('at', 'O'), ('pittsburgh', 'B-city_name')]\n",
      "\n",
      "\n",
      "[('what', 'O'), ('is', 'O'), ('the', 'O'), ('first', 'B-class_type'), ('class', 'I-class_type'), ('fare', 'O'), ('on', 'O'), ('united', 'B-airline_name'), ('flight', 'O'), ('DIGITDIGITDIGIT', 'B-flight_number'), ('from', 'O'), ('denver', 'B-fromloc.city_name'), ('to', 'O'), ('boston', 'B-toloc.city_name')]\n",
      "\n",
      "\n",
      "[('show', 'O'), ('flights', 'O'), ('first', 'B-class_type'), ('class', 'I-class_type'), ('on', 'O'), ('american', 'B-airline_name'), ('from', 'O'), ('dallas', 'B-fromloc.city_name'), ('fort', 'I-fromloc.city_name'), ('worth', 'I-fromloc.city_name'), ('to', 'O'), ('san', 'B-toloc.airport_name'), ('francisco', 'I-toloc.airport_name'), ('international', 'I-toloc.airport_name')]\n",
      "\n",
      "\n",
      "[('list', 'O'), ('the', 'O'), ('northwest', 'B-airline_name'), ('airlines', 'I-airline_name'), ('flights', 'O'), ('that', 'O'), ('leave', 'O'), ('denver', 'B-fromloc.city_name'), ('before', 'B-depart_time.time_relative'), ('noon', 'B-depart_time.time')]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# printing a few randomly chosen sentences and the corresponding labels (tagged entities)\n",
    "for i in random.sample(range(len(train_x)), 20):\n",
    "    w = list(map(lambda x: id_to_words[x], train_x[i]))\n",
    "    l = list(map(lambda x: id_to_labels[x], train_label[i]))\n",
    "    print(list(zip(w, l)))\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's write a function which takes in an index and returns the corresponding query with its labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_query(index):\n",
    "    w = list(map(lambda x: id_to_words[x], train_x[index]))\n",
    "    l = list(map(lambda x: id_to_labels[x], train_label[index]))\n",
    "    s = list(zip(w, l))\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('on', 'O'),\n",
       " ('<UNK>', 'B-airline_name'),\n",
       " ('air', 'I-airline_name'),\n",
       " ('how', 'O'),\n",
       " ('many', 'O'),\n",
       " ('flights', 'O'),\n",
       " ('leaving', 'O'),\n",
       " ('oakland', 'B-fromloc.city_name'),\n",
       " ('on', 'O'),\n",
       " ('july', 'B-depart_date.month_name'),\n",
       " ('twenty', 'B-depart_date.day_number'),\n",
       " ('seventh', 'I-depart_date.day_number'),\n",
       " ('to', 'O'),\n",
       " ('boston', 'B-toloc.city_name'),\n",
       " ('nonstop', 'B-flight_stop')]"
      ]
     },
     "execution_count": 308,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print_query(3925)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, some queries specify stopover cities, such as this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('is', 'O'),\n",
       " ('there', 'O'),\n",
       " ('a', 'O'),\n",
       " ('flight', 'O'),\n",
       " ('between', 'O'),\n",
       " ('oakland', 'B-fromloc.city_name'),\n",
       " ('and', 'O'),\n",
       " ('boston', 'B-toloc.city_name'),\n",
       " ('with', 'O'),\n",
       " ('a', 'O'),\n",
       " ('stopover', 'O'),\n",
       " ('in', 'O'),\n",
       " ('dallas', 'B-stoploc.city_name'),\n",
       " ('fort', 'I-stoploc.city_name'),\n",
       " ('worth', 'I-stoploc.city_name'),\n",
       " ('on', 'O'),\n",
       " ('twa', 'B-airline_code')]"
      ]
     },
     "execution_count": 309,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print_query(3443)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that in this dataset, queries are far more complex (in terms of number of labels, variety in the sentence structures etc.) and thus we cannot  write simple hand-written rules to extract chunks such as to_from_city, types_of_meals etc. \n",
    "\n",
    "Thus, we need to train probabilistic models such as CRFs, HMMs etc. to tag each word with its corresponding entity label.\n",
    "\n",
    "We'll use the training and validation sets ```train_x``` and ```valid_x``` as to tune the model, and finaly use test set to measure the performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models for NER\n",
    "\n",
    "Let's experiment with a few different models for labelling words with named entities.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [],
   "source": [
    "# POS tagging sentences\n",
    "# takes in a list of sentences and returns a list of POS-tagged sentences\n",
    "# in the form (word, tag)\n",
    "\n",
    "def pos_tag(sent_list):\n",
    "    pos_tags = []    \n",
    "    for sent in sent_list:\n",
    "        tagged_words = nltk.pos_tag([id_to_words[val] for val in sent])\n",
    "        pos_tags.append(tagged_words)\n",
    "    return pos_tags\n",
    "\n",
    "train_pos = pos_tag(train_x)\n",
    "valid_pos = pos_tag(val_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('okay', 'NN'),\n",
       " ('could', 'MD'),\n",
       " ('you', 'PRP'),\n",
       " ('get', 'VB'),\n",
       " ('me', 'PRP'),\n",
       " ('a', 'DT'),\n",
       " ('round', 'NN'),\n",
       " ('trip', 'NN'),\n",
       " ('ticket', 'NN'),\n",
       " ('from', 'IN'),\n",
       " ('indianapolis', 'NN'),\n",
       " ('to', 'TO'),\n",
       " ('kansas', 'VB'),\n",
       " ('city', 'NN')]"
      ]
     },
     "execution_count": 311,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# looking at tags of some randomly chosen queries\n",
    "# notice that most cities after 'TO' are tagged as VB\n",
    "i = random.randrange(len(train_pos))\n",
    "train_pos[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train a model, we need the entity labels of each word along with the POS tags, for e.g. in this format:\n",
    "```[('New', 'NNP', u'B-GPE'), ('York', 'NNP', u'I-GPE'), ('is', 'VBZ', u'O'), ('my', 'PRP$', u'O'), ('favorite', 'JJ', u'O'), ('city', 'NN', u'O')]```\n",
    "\n",
    "Let's convert the training and validation sentences to this form. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting each word in train sentences to 3-tuples \n",
    "# of the form (word, tag, IOB_tag)\n",
    "\n",
    "train_labels = []\n",
    "for sent in list(zip(train_pos, train_label)):\n",
    "    pos = sent[0]\n",
    "    labels = sent[1]\n",
    "    l = list(zip(pos, labels))\n",
    "    tuple_3 = [(i[0][0], i[0][1], id_to_labels[i[1]]) for i in l]\n",
    "    train_labels.append(tuple_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('may', 'MD', 'O'),\n",
       " ('i', 'VB', 'O'),\n",
       " ('fly', 'NN', 'O'),\n",
       " ('from', 'IN', 'O'),\n",
       " ('san', 'JJ', 'B-fromloc.city_name'),\n",
       " ('francisco', 'NN', 'I-fromloc.city_name'),\n",
       " ('to', 'TO', 'O'),\n",
       " ('baltimore', 'VB', 'B-toloc.city_name')]"
      ]
     },
     "execution_count": 313,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# some sample training sentences\n",
    "train_labels[random.randrange(len(train_labels))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [],
   "source": [
    "# doing the same for validation and test data\n",
    "valid_labels = []\n",
    "for sent in list(zip(valid_pos, val_label)):\n",
    "    pos = sent[0]\n",
    "    labels = sent[1]\n",
    "    l = list(zip(pos, labels))\n",
    "    tuple_3 = [(i[0][0], i[0][1], id_to_labels[i[1]]) for i in l]\n",
    "    valid_labels.append(tuple_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting to Tree Format\n",
    "\n",
    "Let's now convert the sentences into a tree format, which is needed by NLTK to train taggers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  i/NNS\n",
      "  need/VBP\n",
      "  a/DT\n",
      "  flight/NN\n",
      "  from/IN\n",
      "  (fromloc.city_name atlanta/NN)\n",
      "  to/TO\n",
      "  (toloc.city_name philadelphia/VB)\n",
      "  and/CC\n",
      "  i/VB\n",
      "  'm/VBP\n",
      "  looking/VBG\n",
      "  for/IN\n",
      "  the/DT\n",
      "  (cost_relative cheapest/JJS)\n",
      "  fare/NN)\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import conll2000\n",
    "from nltk import conlltags2tree, tree2conlltags\n",
    "\n",
    "# converting a sample sentence to a tree\n",
    "tree = conlltags2tree(train_labels[2])\n",
    "print(tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now convert all training sentences to trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting training and validation data to tree format\n",
    "train_trees = [conlltags2tree(sent) for sent in train_labels]\n",
    "valid_trees = [conlltags2tree(sent) for sent in valid_labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  show/VB\n",
      "  me/PRP\n",
      "  the/DT\n",
      "  (flight_mod last/JJ)\n",
      "  flight/NN\n",
      "  from/IN\n",
      "  (fromloc.city_name denver/NN)\n",
      "  to/TO\n",
      "  (toloc.city_name boston/VB))\n"
     ]
    }
   ],
   "source": [
    "# print some sample training trees\n",
    "print(train_trees[random.randrange(len(train_trees))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now try building some parsers. \n",
    "\n",
    "### Regex Based Parsers\n",
    "\n",
    "Let's start with a dummy parser - one which tags every token as an 'O'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChunkParse score:\n",
      "    IOB Accuracy:  63.2%%\n",
      "    Precision:      0.0%%\n",
      "    Recall:         0.0%%\n",
      "    F-Measure:      0.0%%\n"
     ]
    }
   ],
   "source": [
    "# a dummy chunk parser - tags every word as 'O'\n",
    "cp = nltk.RegexpParser(r'')\n",
    "print(cp.evaluate(valid_trees))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above results tell us that about 63% of the tokens are tagged as 'O', i.e. they are not a named entity of any type. The precision, recall etc. are zero because we did not find any chunks at all."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unigram Chunker\n",
    "\n",
    "Let's now try a unigram chunker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unigram chunker\n",
    "\n",
    "from nltk import ChunkParserI\n",
    "\n",
    "class UnigramChunker(ChunkParserI):    \n",
    "    def __init__(self, train_sents):\n",
    "        # convert train sents from tree format to tags\n",
    "        train_data = [[(t, c) for w, t, c in nltk.chunk.tree2conlltags(sent)] \n",
    "                      for sent in train_sents]\n",
    "        self.tagger = nltk.UnigramTagger(train_data)\n",
    "        \n",
    "    def parse(self, sentence):\n",
    "        pos_tags = [pos for (word, pos) in sentence]\n",
    "        tagged_pos_tags = self.tagger.tag(pos_tags)\n",
    "        chunktags = [chunktag for (pos, chunktag) in tagged_pos_tags]\n",
    "        \n",
    "        # convert to tree again\n",
    "        conlltags = [(word, pos, chunktag) for ((word, pos), chunktag) in zip(sentence, chunktags)]\n",
    "        return nltk.chunk.conlltags2tree(conlltags)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChunkParse score:\n",
      "    IOB Accuracy:  66.0%%\n",
      "    Precision:     37.4%%\n",
      "    Recall:        19.3%%\n",
      "    F-Measure:     25.5%%\n"
     ]
    }
   ],
   "source": [
    "# unigram chunker \n",
    "unigram_chunker = UnigramChunker(train_trees)\n",
    "print(unigram_chunker.evaluate(valid_trees))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy, precision and recall have of course improved compared to the previous dummy parser. Let's also look at what the unigram parser has learnt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('CC', 'O'), ('CD', 'B-round_trip'), ('DT', 'O'), ('EX', 'O'), ('FW', 'B-fromloc.city_name'), ('IN', 'O'), ('JJ', 'O'), ('JJR', 'B-cost_relative'), ('JJS', 'B-cost_relative'), ('MD', 'O'), ('NN', 'O'), ('NNP', 'B-depart_time.time'), ('NNS', 'O'), ('PDT', 'O'), ('POS', 'O'), ('PRP', 'O'), ('PRP$', 'O'), ('RB', 'O'), ('RBR', 'B-cost_relative'), ('RBS', 'B-cost_relative'), ('RP', 'O'), ('TO', 'O'), ('UH', 'O'), ('VB', 'B-toloc.city_name'), ('VBD', 'O'), ('VBG', 'O'), ('VBN', 'O'), ('VBP', 'O'), ('VBZ', 'O'), ('WDT', 'O'), ('WP', 'O'), ('WRB', 'O')]\n"
     ]
    }
   ],
   "source": [
    "# printing the most likely IOB tags for each POS tag\n",
    "\n",
    "# extract the list of pos tags\n",
    "postags = sorted(set([pos for sent in train_trees for (word, pos) in sent.leaves()]))\n",
    "\n",
    "# for each tag, assign the most likely IOB label\n",
    "print(unigram_chunker.tagger.tag(postags))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The unigram tagger has learnt that most pos tags are indeed an 'O', i.e. don't form an entity. Some interesting patterns it has learnt are:\n",
    "- JJR, JJS (relative adjectives), are most likely B-cost_relative (e.g. cheapest, cheaper)\n",
    "- NNP is most likely to be B-depart_time.time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bigram Chunker\n",
    "\n",
    "Let's try a bigram chunker as well - we just need to change the ```UnigramTagger``` to ```BigramTagger```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bigram tagger\n",
    "\n",
    "class BigramChunker(ChunkParserI):    \n",
    "    def __init__(self, train_sents):\n",
    "        # convert train sents from tree format to tags\n",
    "        train_data = [[(t, c) for w, t, c in nltk.chunk.tree2conlltags(sent)] \n",
    "                      for sent in train_sents]\n",
    "        self.tagger = nltk.BigramTagger(train_data)\n",
    "        \n",
    "    def parse(self, sentence):\n",
    "        pos_tags = [pos for (word, pos) in sentence]\n",
    "        tagged_pos_tags = self.tagger.tag(pos_tags)\n",
    "        chunktags = [chunktag for (pos, chunktag) in tagged_pos_tags]\n",
    "        \n",
    "        # convert to tree again\n",
    "        conlltags = [(word, pos, chunktag) for ((word, pos), chunktag) in zip(sentence, chunktags)]\n",
    "        return nltk.chunk.conlltags2tree(conlltags)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChunkParse score:\n",
      "    IOB Accuracy:  70.9%%\n",
      "    Precision:     45.8%%\n",
      "    Recall:        40.5%%\n",
      "    F-Measure:     43.0%%\n"
     ]
    }
   ],
   "source": [
    "# unigram chunker \n",
    "bigram_chunker = BigramChunker(train_trees)\n",
    "print(bigram_chunker.evaluate(valid_trees))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The metrics have improved significantly from unigram to bigram."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifier Based Chunkers\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConsecutiveNPChunkTagger(nltk.TaggerI): \n",
    "\n",
    "    def __init__(self, train_sents):\n",
    "        train_set = []\n",
    "        for tagged_sent in train_sents:\n",
    "            untagged_sent = nltk.tag.untag(tagged_sent)\n",
    "            history = []\n",
    "            for i, (word, tag) in enumerate(tagged_sent):\n",
    "                featureset = npchunk_features(untagged_sent, i, history) \n",
    "                train_set.append( (featureset, tag) )\n",
    "                history.append(tag)\n",
    "        self.classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "\n",
    "    def tag(self, sentence):\n",
    "        history = []\n",
    "        for i, word in enumerate(sentence):\n",
    "            featureset = npchunk_features(sentence, i, history)\n",
    "            tag = self.classifier.classify(featureset)\n",
    "            history.append(tag)\n",
    "        return zip(sentence, history)\n",
    "\n",
    "class ConsecutiveNPChunker(nltk.ChunkParserI): \n",
    "    def __init__(self, train_sents):\n",
    "        tagged_sents = [[((w,t),c) for (w,t,c) in\n",
    "                         nltk.chunk.tree2conlltags(sent)]\n",
    "                        for sent in train_sents]\n",
    "        self.tagger = ConsecutiveNPChunkTagger(tagged_sents)\n",
    "\n",
    "    def parse(self, sentence):\n",
    "        tagged_sents = self.tagger.tag(sentence)\n",
    "        conlltags = [(w,t,c) for ((w,t),c) in tagged_sents]\n",
    "        return nltk.chunk.conlltags2tree(conlltags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extracts features for a given word i in a given sentence \n",
    "# history refers to the previous POS tags in the sentence\n",
    "def npchunk_features(sentence, i, history):\n",
    "    word, pos = sentence[i]\n",
    "    \n",
    "    # the first word has both previous word and previous tag undefined\n",
    "    if i == 0:\n",
    "        prevword, prevpos = \"<START>\", \"<START>\"\n",
    "    else:\n",
    "        prevword, prevpos = sentence[i-1]\n",
    "\n",
    "    # gazetteer lookup features (see section below)\n",
    "    gazetteer = gazetteer_lookup(word)\n",
    "\n",
    "    return {\"pos\": pos, \"prevpos\": prevpos, 'word':word,\n",
    "           'word_is_city': gazetteer[0],\n",
    "           'word_is_state': gazetteer[1],\n",
    "           'word_is_county': gazetteer[2]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('what', 'WP'),\n",
       " ('aircraft', 'NN'),\n",
       " ('is', 'VBZ'),\n",
       " ('used', 'VBN'),\n",
       " ('on', 'IN'),\n",
       " ('delta', 'JJ'),\n",
       " ('flight', 'NN'),\n",
       " ('DIGITDIGITDIGITDIGIT', 'NNP'),\n",
       " ('from', 'IN'),\n",
       " ('kansas', 'NNP'),\n",
       " ('city', 'NN'),\n",
       " ('to', 'TO'),\n",
       " ('salt', 'VB'),\n",
       " ('lake', 'JJ'),\n",
       " ('city', 'NN')]"
      ]
     },
     "execution_count": 326,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example features for a given sentence\n",
    "sent_pos = train_pos[0]\n",
    "sent_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'pos': 'WP', 'prevpos': '<START>', 'word': 'what', 'word_is_city': False, 'word_is_state': False, 'word_is_county': False}\n",
      " \n",
      "{'pos': 'NN', 'prevpos': 'WP', 'word': 'aircraft', 'word_is_city': False, 'word_is_state': False, 'word_is_county': False}\n",
      " \n",
      "{'pos': 'VBZ', 'prevpos': 'NN', 'word': 'is', 'word_is_city': False, 'word_is_state': False, 'word_is_county': False}\n",
      " \n",
      "{'pos': 'VBN', 'prevpos': 'VBZ', 'word': 'used', 'word_is_city': False, 'word_is_state': False, 'word_is_county': False}\n",
      " \n",
      "{'pos': 'IN', 'prevpos': 'VBN', 'word': 'on', 'word_is_city': False, 'word_is_state': False, 'word_is_county': False}\n",
      " \n",
      "{'pos': 'JJ', 'prevpos': 'IN', 'word': 'delta', 'word_is_city': True, 'word_is_state': False, 'word_is_county': True}\n",
      " \n",
      "{'pos': 'NN', 'prevpos': 'JJ', 'word': 'flight', 'word_is_city': False, 'word_is_state': False, 'word_is_county': False}\n",
      " \n",
      "{'pos': 'NNP', 'prevpos': 'NN', 'word': 'DIGITDIGITDIGITDIGIT', 'word_is_city': False, 'word_is_state': False, 'word_is_county': False}\n",
      " \n",
      "{'pos': 'IN', 'prevpos': 'NNP', 'word': 'from', 'word_is_city': False, 'word_is_state': False, 'word_is_county': False}\n",
      " \n",
      "{'pos': 'NNP', 'prevpos': 'IN', 'word': 'kansas', 'word_is_city': True, 'word_is_state': True, 'word_is_county': False}\n",
      " \n",
      "{'pos': 'NN', 'prevpos': 'NNP', 'word': 'city', 'word_is_city': False, 'word_is_state': False, 'word_is_county': False}\n",
      " \n",
      "{'pos': 'TO', 'prevpos': 'NN', 'word': 'to', 'word_is_city': False, 'word_is_state': False, 'word_is_county': False}\n",
      " \n",
      "{'pos': 'VB', 'prevpos': 'TO', 'word': 'salt', 'word_is_city': False, 'word_is_state': False, 'word_is_county': False}\n",
      " \n",
      "{'pos': 'JJ', 'prevpos': 'VB', 'word': 'lake', 'word_is_city': True, 'word_is_state': False, 'word_is_county': True}\n",
      " \n",
      "{'pos': 'NN', 'prevpos': 'JJ', 'word': 'city', 'word_is_city': False, 'word_is_state': False, 'word_is_county': False}\n",
      " \n"
     ]
    }
   ],
   "source": [
    "# example features for a sentence\n",
    "for i in range(len(sent_pos)):\n",
    "    print(npchunk_features(sent_pos, i, history=[]))\n",
    "    print(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training the chunker \n",
    "chunker = ConsecutiveNPChunker(train_trees)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChunkParse score:\n",
      "    IOB Accuracy:  91.6%%\n",
      "    Precision:     75.6%%\n",
      "    Recall:        82.3%%\n",
      "    F-Measure:     78.8%%\n"
     ]
    }
   ],
   "source": [
    "# evaluate the chunker\n",
    "print(chunker.evaluate(valid_trees))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results have improved significantly compared to the basic unigram/bigram chunkers, and they may improve further if we create better features.\n",
    "\n",
    "For example, if the word is 'DIGIT' (numbers are labelled as 'DIGIT' in this dataset), we can have a feature which indicates that (see example below). In this dataset, 4-digit numbers are encoded as 'DIGITDIGITDIGITDIGIT'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('do', 'VBP'),\n",
       " ('you', 'PRP'),\n",
       " ('have', 'VB'),\n",
       " ('an', 'DT'),\n",
       " ('DIGITDIGITDIGIT', 'NNP'),\n",
       " ('flight', 'NN'),\n",
       " ('from', 'IN'),\n",
       " ('denver', 'NN'),\n",
       " ('to', 'TO'),\n",
       " ('san', 'VB'),\n",
       " ('francisco', 'NN')]"
      ]
     },
     "execution_count": 330,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example of 'DIGITDIGIT'\n",
    "train_pos[1326]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's add some of these features and see if the performance improves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extracts features for a given word i in a given sentence \n",
    "# history refers to the previous POS tags in the sentence\n",
    "def npchunk_features(sentence, i, history):\n",
    "    word, pos = sentence[i]\n",
    "    \n",
    "    # the first word has both previous word and previous tag undefined\n",
    "    if i == 0:\n",
    "        prevword, prevpos = \"<START>\", \"<START>\"\n",
    "    else:\n",
    "        prevword, prevpos = sentence[i-1]\n",
    "        \n",
    "    if i == len(sentence)-1:\n",
    "        nextword, nextpos = '<END>', '<END>'\n",
    "    else:\n",
    "        nextword, nextpos = sentence[i+1]\n",
    "\n",
    "    # gazetteer lookup features (see section below)\n",
    "    gazetteer = gazetteer_lookup(word)\n",
    "\n",
    "    # adding word_is_digit feature (boolean)\n",
    "    return {\"pos\": pos, \"prevpos\": prevpos, 'word':word, \n",
    "           'word_is_city': gazetteer[0],\n",
    "           'word_is_state': gazetteer[1],\n",
    "           'word_is_county': gazetteer[2],\n",
    "           'word_is_digit': word in 'DIGITDIGITDIGIT', \n",
    "           'nextword': nextword, \n",
    "           'nextpos': nextpos}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChunkParse score:\n",
      "    IOB Accuracy:  91.6%%\n",
      "    Precision:     75.6%%\n",
      "    Recall:        84.7%%\n",
      "    F-Measure:     79.9%%\n"
     ]
    }
   ],
   "source": [
    "# train and evaluate the chunker \n",
    "chunker = ConsecutiveNPChunker(train_trees)\n",
    "print(chunker.evaluate(valid_trees))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ChunkParse score:\n",
    "#     IOB Accuracy:  92.7%%\n",
    "#     Precision:     78.1%%\n",
    "#     Recall:        84.9%%\n",
    "#     F-Measure:     81.4%%\n",
    "\n",
    "# ChunkParse score:\n",
    "#     IOB Accuracy:  91.7%%\n",
    "#     Precision:     75.5%%\n",
    "#     Recall:        82.0%%\n",
    "#     F-Measure:     78.6%%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using a Gazetteer to Lookup Cities and States\n",
    "\n",
    "URL: https://raw.githubusercontent.com/grammakov/USA-cities-and-states/master/us_cities_states_counties.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>City</th>\n",
       "      <th>State short</th>\n",
       "      <th>State full</th>\n",
       "      <th>County</th>\n",
       "      <th>City alias</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Holtsville</td>\n",
       "      <td>NY</td>\n",
       "      <td>New York</td>\n",
       "      <td>SUFFOLK</td>\n",
       "      <td>Internal Revenue Service</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Holtsville</td>\n",
       "      <td>NY</td>\n",
       "      <td>New York</td>\n",
       "      <td>SUFFOLK</td>\n",
       "      <td>Holtsville</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Adjuntas</td>\n",
       "      <td>PR</td>\n",
       "      <td>Puerto Rico</td>\n",
       "      <td>ADJUNTAS</td>\n",
       "      <td>URB San Joaquin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Adjuntas</td>\n",
       "      <td>PR</td>\n",
       "      <td>Puerto Rico</td>\n",
       "      <td>ADJUNTAS</td>\n",
       "      <td>Jard De Adjuntas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Adjuntas</td>\n",
       "      <td>PR</td>\n",
       "      <td>Puerto Rico</td>\n",
       "      <td>ADJUNTAS</td>\n",
       "      <td>Colinas Del Gigante</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         City State short   State full    County                City alias\n",
       "0  Holtsville          NY     New York   SUFFOLK  Internal Revenue Service\n",
       "1  Holtsville          NY     New York   SUFFOLK                Holtsville\n",
       "2    Adjuntas          PR  Puerto Rico  ADJUNTAS           URB San Joaquin\n",
       "3    Adjuntas          PR  Puerto Rico  ADJUNTAS          Jard De Adjuntas\n",
       "4    Adjuntas          PR  Puerto Rico  ADJUNTAS       Colinas Del Gigante"
      ]
     },
     "execution_count": 334,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reading a file containing list of US cities, states and counties\n",
    "us_cities = pd.read_csv(\"us_cities_states_counties.csv\", sep=\"|\")\n",
    "us_cities.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# storing cities, states and counties as sets\n",
    "cities = set(us_cities['City'].str.lower())\n",
    "states = set(us_cities['State full'].str.lower())\n",
    "counties = set(us_cities['County'].str.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18854\n",
      "62\n",
      "1932\n"
     ]
    }
   ],
   "source": [
    "print(len(cities))\n",
    "print(len(states))\n",
    "print(len(counties))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function to look up a given word in cities, states, county\n",
    "def gazetteer_lookup(word):\n",
    "    return (word in cities, word in states, word in counties)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(True, True, True)\n",
      "(False, True, True)\n",
      "(True, False, True)\n"
     ]
    }
   ],
   "source": [
    "# sample lookups\n",
    "print(gazetteer_lookup('washington'))\n",
    "print(gazetteer_lookup('utah'))\n",
    "print(gazetteer_lookup('philadelphia'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CRF Based Taggers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.19.1\n"
     ]
    }
   ],
   "source": [
    "from itertools import chain\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "import sklearn\n",
    "import pycrfsuite\n",
    "\n",
    "print(sklearn.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('what', 'WP', 'O'),\n",
       " ('aircraft', 'NN', 'O'),\n",
       " ('is', 'VBZ', 'O'),\n",
       " ('used', 'VBN', 'O'),\n",
       " ('on', 'IN', 'O'),\n",
       " ('delta', 'JJ', 'B-airline_name'),\n",
       " ('flight', 'NN', 'O'),\n",
       " ('DIGITDIGITDIGITDIGIT', 'NNP', 'B-flight_number'),\n",
       " ('from', 'IN', 'O'),\n",
       " ('kansas', 'NNP', 'B-fromloc.city_name'),\n",
       " ('city', 'NN', 'I-fromloc.city_name'),\n",
       " ('to', 'TO', 'O'),\n",
       " ('salt', 'VB', 'B-toloc.city_name'),\n",
       " ('lake', 'JJ', 'I-toloc.city_name'),\n",
       " ('city', 'NN', 'I-toloc.city_name')]"
      ]
     },
     "execution_count": 340,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# structure of train/validation data\n",
    "train_labels[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define a function to extract features from a given sentence. This is similar to the ```npchunk_features()``` function defined above, but we'll add some new features as well such as the suffix of the word (upto the last 4 characters), prefix (upto first 4 characters) etc.\n",
    "\n",
    "The list of features we'll extract is as follows:\n",
    "```\n",
    "{\n",
    "            'word':word,\n",
    "            'pos': pos, \n",
    "            'prevword': prevword,\n",
    "            'prevpos': prevpos,  \n",
    "            'nextword': nextword, \n",
    "            'nextpos': nextpos,\n",
    "            'word_is_city': gazetteer[0],\n",
    "            'word_is_state': gazetteer[1],\n",
    "            'word_is_county': gazetteer[2],\n",
    "            'word_is_digit': word in 'DIGITDIGITDIGIT',\n",
    "            'suff_1': suff_1,  \n",
    "            'suff_2': suff_2,  \n",
    "            'suff_3': suff_3,  \n",
    "            'suff_4': suff_4, \n",
    "            'pref_1': pref_1,  \n",
    "            'pref_2': pref_2,  \n",
    "            'pref_3': pref_3, \n",
    "            'pref_4': pref_4 \n",
    "\n",
    "}\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [],
   "source": [
    "## other features to consider\n",
    "\n",
    "# airline code\n",
    "# airline name\n",
    "# day name (monday/tuesday etc.) i=1847, 2769\n",
    "# o'clock (word shape): i=379\n",
    "\n",
    "# i=random.randrange(len(train_labels))\n",
    "# train_labels[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract features from a given sentence\n",
    "def word_features(sent, i):\n",
    "    word = sent[i][0]\n",
    "    pos = sent[i][1]\n",
    "    \n",
    "    # first word\n",
    "    if i==0:\n",
    "        prevword = '<START>'\n",
    "        prevpos = '<START>'\n",
    "    else:\n",
    "        prevword = sent[i-1][0]\n",
    "        prevpos = sent[i-1][1]\n",
    "    \n",
    "    # last word\n",
    "    if i == len(sent)-1:\n",
    "        nextword = '<END>'\n",
    "        nextpos = '<END>'\n",
    "    else:\n",
    "        nextword = sent[i+1][0]\n",
    "        nextpos = sent[i+1][1]\n",
    "    \n",
    "    # word is in gazetteer\n",
    "    gazetteer = gazetteer_lookup(word)\n",
    "    \n",
    "    # suffixes and prefixes\n",
    "    suff_1, suff_2, suff_3, suff_4 = word[:1], word[:2], word[:3], word[:4]\n",
    "    pref_1, pref_2, pref_3, pref_4 = word[-1:], word[-2:], word[-3:], word[-4:]\n",
    "    \n",
    "    return {'word':word,\n",
    "            'pos': pos, \n",
    "            'prevword': prevword,\n",
    "            'prevpos': prevpos,  \n",
    "            'nextword': nextword, \n",
    "            'nextpos': nextpos,\n",
    "            'word_is_city': gazetteer[0],\n",
    "            'word_is_state': gazetteer[1],\n",
    "            'word_is_county': gazetteer[2],\n",
    "            'word_is_digit': word in 'DIGITDIGITDIGIT',\n",
    "            'suff_1': suff_1,  \n",
    "            'suff_2': suff_2,  \n",
    "            'suff_3': suff_3,  \n",
    "            'suff_4': suff_4, \n",
    "            'pref_1': pref_1,  \n",
    "            'pref_2': pref_2,  \n",
    "            'pref_3': pref_3, \n",
    "            'pref_4': pref_4 }  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'nextpos': 'IN',\n",
       " 'nextword': 'on',\n",
       " 'pos': 'VBN',\n",
       " 'pref_1': 'd',\n",
       " 'pref_2': 'ed',\n",
       " 'pref_3': 'sed',\n",
       " 'pref_4': 'used',\n",
       " 'prevpos': 'VBZ',\n",
       " 'prevword': 'is',\n",
       " 'suff_1': 'u',\n",
       " 'suff_2': 'us',\n",
       " 'suff_3': 'use',\n",
       " 'suff_4': 'used',\n",
       " 'word': 'used',\n",
       " 'word_is_city': False,\n",
       " 'word_is_county': False,\n",
       " 'word_is_digit': False,\n",
       " 'word_is_state': False}"
      ]
     },
     "execution_count": 343,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example features\n",
    "word_features(train_labels[0], i=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining a few more functions to extract featrues, labels, words from sentences\n",
    "\n",
    "def sent2features(sent):\n",
    "    return [word_features(sent, i) for i in range(len(sent))]\n",
    "\n",
    "def sent2labels(sent):\n",
    "    return [label for token, postag, label in sent]\n",
    "\n",
    "def sent2tokens(sent):\n",
    "    return [token for token, postag, label in sent]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create training and validation sets\n",
    "X_train = [sent2features(s) for s in train_labels]\n",
    "y_train = [sent2labels(s) for s in train_labels]\n",
    "\n",
    "X_valid = [sent2features(s) for s in valid_labels]\n",
    "y_valid = [sent2labels(s) for s in valid_labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'nextpos': 'NN',\n",
       "  'nextword': 'aircraft',\n",
       "  'pos': 'WP',\n",
       "  'pref_1': 't',\n",
       "  'pref_2': 'at',\n",
       "  'pref_3': 'hat',\n",
       "  'pref_4': 'what',\n",
       "  'prevpos': '<START>',\n",
       "  'prevword': '<START>',\n",
       "  'suff_1': 'w',\n",
       "  'suff_2': 'wh',\n",
       "  'suff_3': 'wha',\n",
       "  'suff_4': 'what',\n",
       "  'word': 'what',\n",
       "  'word_is_city': False,\n",
       "  'word_is_county': False,\n",
       "  'word_is_digit': False,\n",
       "  'word_is_state': False},\n",
       " {'nextpos': 'VBZ',\n",
       "  'nextword': 'is',\n",
       "  'pos': 'NN',\n",
       "  'pref_1': 't',\n",
       "  'pref_2': 'ft',\n",
       "  'pref_3': 'aft',\n",
       "  'pref_4': 'raft',\n",
       "  'prevpos': 'WP',\n",
       "  'prevword': 'what',\n",
       "  'suff_1': 'a',\n",
       "  'suff_2': 'ai',\n",
       "  'suff_3': 'air',\n",
       "  'suff_4': 'airc',\n",
       "  'word': 'aircraft',\n",
       "  'word_is_city': False,\n",
       "  'word_is_county': False,\n",
       "  'word_is_digit': False,\n",
       "  'word_is_state': False},\n",
       " {'nextpos': 'VBN',\n",
       "  'nextword': 'used',\n",
       "  'pos': 'VBZ',\n",
       "  'pref_1': 's',\n",
       "  'pref_2': 'is',\n",
       "  'pref_3': 'is',\n",
       "  'pref_4': 'is',\n",
       "  'prevpos': 'NN',\n",
       "  'prevword': 'aircraft',\n",
       "  'suff_1': 'i',\n",
       "  'suff_2': 'is',\n",
       "  'suff_3': 'is',\n",
       "  'suff_4': 'is',\n",
       "  'word': 'is',\n",
       "  'word_is_city': False,\n",
       "  'word_is_county': False,\n",
       "  'word_is_digit': False,\n",
       "  'word_is_state': False},\n",
       " {'nextpos': 'IN',\n",
       "  'nextword': 'on',\n",
       "  'pos': 'VBN',\n",
       "  'pref_1': 'd',\n",
       "  'pref_2': 'ed',\n",
       "  'pref_3': 'sed',\n",
       "  'pref_4': 'used',\n",
       "  'prevpos': 'VBZ',\n",
       "  'prevword': 'is',\n",
       "  'suff_1': 'u',\n",
       "  'suff_2': 'us',\n",
       "  'suff_3': 'use',\n",
       "  'suff_4': 'used',\n",
       "  'word': 'used',\n",
       "  'word_is_city': False,\n",
       "  'word_is_county': False,\n",
       "  'word_is_digit': False,\n",
       "  'word_is_state': False},\n",
       " {'nextpos': 'JJ',\n",
       "  'nextword': 'delta',\n",
       "  'pos': 'IN',\n",
       "  'pref_1': 'n',\n",
       "  'pref_2': 'on',\n",
       "  'pref_3': 'on',\n",
       "  'pref_4': 'on',\n",
       "  'prevpos': 'VBN',\n",
       "  'prevword': 'used',\n",
       "  'suff_1': 'o',\n",
       "  'suff_2': 'on',\n",
       "  'suff_3': 'on',\n",
       "  'suff_4': 'on',\n",
       "  'word': 'on',\n",
       "  'word_is_city': False,\n",
       "  'word_is_county': False,\n",
       "  'word_is_digit': False,\n",
       "  'word_is_state': False},\n",
       " {'nextpos': 'NN',\n",
       "  'nextword': 'flight',\n",
       "  'pos': 'JJ',\n",
       "  'pref_1': 'a',\n",
       "  'pref_2': 'ta',\n",
       "  'pref_3': 'lta',\n",
       "  'pref_4': 'elta',\n",
       "  'prevpos': 'IN',\n",
       "  'prevword': 'on',\n",
       "  'suff_1': 'd',\n",
       "  'suff_2': 'de',\n",
       "  'suff_3': 'del',\n",
       "  'suff_4': 'delt',\n",
       "  'word': 'delta',\n",
       "  'word_is_city': True,\n",
       "  'word_is_county': True,\n",
       "  'word_is_digit': False,\n",
       "  'word_is_state': False},\n",
       " {'nextpos': 'NNP',\n",
       "  'nextword': 'DIGITDIGITDIGITDIGIT',\n",
       "  'pos': 'NN',\n",
       "  'pref_1': 't',\n",
       "  'pref_2': 'ht',\n",
       "  'pref_3': 'ght',\n",
       "  'pref_4': 'ight',\n",
       "  'prevpos': 'JJ',\n",
       "  'prevword': 'delta',\n",
       "  'suff_1': 'f',\n",
       "  'suff_2': 'fl',\n",
       "  'suff_3': 'fli',\n",
       "  'suff_4': 'flig',\n",
       "  'word': 'flight',\n",
       "  'word_is_city': False,\n",
       "  'word_is_county': False,\n",
       "  'word_is_digit': False,\n",
       "  'word_is_state': False},\n",
       " {'nextpos': 'IN',\n",
       "  'nextword': 'from',\n",
       "  'pos': 'NNP',\n",
       "  'pref_1': 'T',\n",
       "  'pref_2': 'IT',\n",
       "  'pref_3': 'GIT',\n",
       "  'pref_4': 'IGIT',\n",
       "  'prevpos': 'NN',\n",
       "  'prevword': 'flight',\n",
       "  'suff_1': 'D',\n",
       "  'suff_2': 'DI',\n",
       "  'suff_3': 'DIG',\n",
       "  'suff_4': 'DIGI',\n",
       "  'word': 'DIGITDIGITDIGITDIGIT',\n",
       "  'word_is_city': False,\n",
       "  'word_is_county': False,\n",
       "  'word_is_digit': False,\n",
       "  'word_is_state': False},\n",
       " {'nextpos': 'NNP',\n",
       "  'nextword': 'kansas',\n",
       "  'pos': 'IN',\n",
       "  'pref_1': 'm',\n",
       "  'pref_2': 'om',\n",
       "  'pref_3': 'rom',\n",
       "  'pref_4': 'from',\n",
       "  'prevpos': 'NNP',\n",
       "  'prevword': 'DIGITDIGITDIGITDIGIT',\n",
       "  'suff_1': 'f',\n",
       "  'suff_2': 'fr',\n",
       "  'suff_3': 'fro',\n",
       "  'suff_4': 'from',\n",
       "  'word': 'from',\n",
       "  'word_is_city': False,\n",
       "  'word_is_county': False,\n",
       "  'word_is_digit': False,\n",
       "  'word_is_state': False},\n",
       " {'nextpos': 'NN',\n",
       "  'nextword': 'city',\n",
       "  'pos': 'NNP',\n",
       "  'pref_1': 's',\n",
       "  'pref_2': 'as',\n",
       "  'pref_3': 'sas',\n",
       "  'pref_4': 'nsas',\n",
       "  'prevpos': 'IN',\n",
       "  'prevword': 'from',\n",
       "  'suff_1': 'k',\n",
       "  'suff_2': 'ka',\n",
       "  'suff_3': 'kan',\n",
       "  'suff_4': 'kans',\n",
       "  'word': 'kansas',\n",
       "  'word_is_city': True,\n",
       "  'word_is_county': False,\n",
       "  'word_is_digit': False,\n",
       "  'word_is_state': True},\n",
       " {'nextpos': 'TO',\n",
       "  'nextword': 'to',\n",
       "  'pos': 'NN',\n",
       "  'pref_1': 'y',\n",
       "  'pref_2': 'ty',\n",
       "  'pref_3': 'ity',\n",
       "  'pref_4': 'city',\n",
       "  'prevpos': 'NNP',\n",
       "  'prevword': 'kansas',\n",
       "  'suff_1': 'c',\n",
       "  'suff_2': 'ci',\n",
       "  'suff_3': 'cit',\n",
       "  'suff_4': 'city',\n",
       "  'word': 'city',\n",
       "  'word_is_city': False,\n",
       "  'word_is_county': False,\n",
       "  'word_is_digit': False,\n",
       "  'word_is_state': False},\n",
       " {'nextpos': 'VB',\n",
       "  'nextword': 'salt',\n",
       "  'pos': 'TO',\n",
       "  'pref_1': 'o',\n",
       "  'pref_2': 'to',\n",
       "  'pref_3': 'to',\n",
       "  'pref_4': 'to',\n",
       "  'prevpos': 'NN',\n",
       "  'prevword': 'city',\n",
       "  'suff_1': 't',\n",
       "  'suff_2': 'to',\n",
       "  'suff_3': 'to',\n",
       "  'suff_4': 'to',\n",
       "  'word': 'to',\n",
       "  'word_is_city': False,\n",
       "  'word_is_county': False,\n",
       "  'word_is_digit': False,\n",
       "  'word_is_state': False},\n",
       " {'nextpos': 'JJ',\n",
       "  'nextword': 'lake',\n",
       "  'pos': 'VB',\n",
       "  'pref_1': 't',\n",
       "  'pref_2': 'lt',\n",
       "  'pref_3': 'alt',\n",
       "  'pref_4': 'salt',\n",
       "  'prevpos': 'TO',\n",
       "  'prevword': 'to',\n",
       "  'suff_1': 's',\n",
       "  'suff_2': 'sa',\n",
       "  'suff_3': 'sal',\n",
       "  'suff_4': 'salt',\n",
       "  'word': 'salt',\n",
       "  'word_is_city': False,\n",
       "  'word_is_county': False,\n",
       "  'word_is_digit': False,\n",
       "  'word_is_state': False},\n",
       " {'nextpos': 'NN',\n",
       "  'nextword': 'city',\n",
       "  'pos': 'JJ',\n",
       "  'pref_1': 'e',\n",
       "  'pref_2': 'ke',\n",
       "  'pref_3': 'ake',\n",
       "  'pref_4': 'lake',\n",
       "  'prevpos': 'VB',\n",
       "  'prevword': 'salt',\n",
       "  'suff_1': 'l',\n",
       "  'suff_2': 'la',\n",
       "  'suff_3': 'lak',\n",
       "  'suff_4': 'lake',\n",
       "  'word': 'lake',\n",
       "  'word_is_city': True,\n",
       "  'word_is_county': True,\n",
       "  'word_is_digit': False,\n",
       "  'word_is_state': False},\n",
       " {'nextpos': '<END>',\n",
       "  'nextword': '<END>',\n",
       "  'pos': 'NN',\n",
       "  'pref_1': 'y',\n",
       "  'pref_2': 'ty',\n",
       "  'pref_3': 'ity',\n",
       "  'pref_4': 'city',\n",
       "  'prevpos': 'JJ',\n",
       "  'prevword': 'lake',\n",
       "  'suff_1': 'c',\n",
       "  'suff_2': 'ci',\n",
       "  'suff_3': 'cit',\n",
       "  'suff_4': 'city',\n",
       "  'word': 'city',\n",
       "  'word_is_city': False,\n",
       "  'word_is_county': False,\n",
       "  'word_is_digit': False,\n",
       "  'word_is_state': False}]"
      ]
     },
     "execution_count": 346,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# X_train is a list of sentences within which each feature has a corresponding dict of features\n",
    "# first sentence in X_train\n",
    "X_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate a CRF trainer from pycrfsuite\n",
    "trainer = pycrfsuite.Trainer(verbose=False)\n",
    "\n",
    "# zip the X and y sets\n",
    "for xseq, yseq in zip(X_train, y_train):\n",
    "    trainer.append(xseq, yseq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set training parameters - using L-BFGS training algorithm (default) with Elastic Net (L1 + L2) regularization.\n",
    "trainer.set_params({\n",
    "    'c1': 1.0,   # coefficient for L1 penalty\n",
    "    'c2': 1e-3,  # coefficient for L2 penalty\n",
    "    'max_iterations': 50,  # stop earlier\n",
    "\n",
    "    # include transitions that are possible, but not observed\n",
    "    'feature.possible_transitions': True\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['feature.minfreq',\n",
       " 'feature.possible_states',\n",
       " 'feature.possible_transitions',\n",
       " 'c1',\n",
       " 'c2',\n",
       " 'max_iterations',\n",
       " 'num_memories',\n",
       " 'epsilon',\n",
       " 'period',\n",
       " 'delta',\n",
       " 'linesearch',\n",
       " 'max_linesearch']"
      ]
     },
     "execution_count": 349,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# list of possible params\n",
    "trainer.params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving the trained model to a file\n",
    "trainer.train('atis.crfsuite')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<contextlib.closing at 0x11c5aa30>"
      ]
     },
     "execution_count": 351,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a tagger object and open the trained file\n",
    "tagger = pycrfsuite.Tagger()\n",
    "tagger.open('atis.crfsuite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "what flights leave atlanta at about DIGIT in the afternoon and arrive in san francisco\n"
     ]
    }
   ],
   "source": [
    "# tagging a sample sentence\n",
    "sample_sent = valid_labels[0]\n",
    "print(' '.join(sent2tokens(sample_sent)), end='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: O O O B-fromloc.city_name O O O O O B-depart_time.period_of_day O O O B-toloc.city_name I-toloc.city_name\n",
      "\n",
      "\n",
      "Correct:   O O O B-fromloc.city_name O B-depart_time.time_relative B-depart_time.time O O B-depart_time.period_of_day O O O B-toloc.city_name I-toloc.city_name\n"
     ]
    }
   ],
   "source": [
    "print(\"Predicted:\", ' '.join(tagger.tag(sent2features(sample_sent))))\n",
    "print('\\n')\n",
    "print(\"Correct:  \", ' '.join(sent2labels(sample_sent)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iob_classification_report(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Classification report for a list of IOB-encoded sequences.\n",
    "    It computes token-level metrics and discards \"O\" labels.\n",
    "\n",
    "    \"\"\"\n",
    "    lb = LabelBinarizer()\n",
    "    y_true_combined = lb.fit_transform(list(chain.from_iterable(y_true)))\n",
    "    y_pred_combined = lb.transform(list(chain.from_iterable(y_pred)))\n",
    "        \n",
    "    # note that we are not including 'O' as a class\n",
    "    tagset = set(lb.classes_) - {'O'}\n",
    "    tagset = sorted(tagset, key=lambda tag: tag.split('-', 1)[::-1])\n",
    "    class_indices = {cls: idx for idx, cls in enumerate(lb.classes_)}\n",
    "    \n",
    "    return classification_report(\n",
    "        y_true_combined,\n",
    "        y_pred_combined,\n",
    "        labels = [class_indices[cls] for cls in tagset],\n",
    "        target_names = tagset,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-fromloc.city_name',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-depart_time.period_of_day',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-toloc.city_name',\n",
       " 'I-toloc.city_name']"
      ]
     },
     "execution_count": 355,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = [tagger.tag(xseq) for xseq in X_valid]\n",
    "\n",
    "# predictions for first sentence\n",
    "y_pred[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now evaluate the model. Since we are dealing with a multiclass classification problem, we can use sklearn's ```LabelBinarizer``` to binarize labels in a one-versus-all manner.  \n",
    "\n",
    "Read about LabelBinarizer here: http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelBinarizer.html."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                              precision    recall  f1-score   support\n",
      "\n",
      "             B-aircraft_code       1.00      0.44      0.62         9\n",
      "              B-airline_code       0.83      0.79      0.81        19\n",
      "              B-airline_name       1.00      0.98      0.99       141\n",
      "              I-airline_name       0.99      1.00      0.99        79\n",
      "              B-airport_code       0.90      1.00      0.95         9\n",
      "              B-airport_name       0.71      0.56      0.63         9\n",
      "              I-airport_name       0.71      0.45      0.56        11\n",
      " B-arrive_date.date_relative       0.00      0.00      0.00         1\n",
      "      B-arrive_date.day_name       0.40      0.11      0.17        18\n",
      "    B-arrive_date.day_number       0.33      0.40      0.36         5\n",
      "    I-arrive_date.day_number       0.33      1.00      0.50         1\n",
      "    B-arrive_date.month_name       0.33      0.40      0.36         5\n",
      "      B-arrive_time.end_time       0.00      0.00      0.00         1\n",
      "      I-arrive_time.end_time       0.00      0.00      0.00         1\n",
      "    B-arrive_time.period_mod       0.00      0.00      0.00         1\n",
      " B-arrive_time.period_of_day       0.50      0.12      0.20         8\n",
      " I-arrive_time.period_of_day       0.00      0.00      0.00         1\n",
      "    B-arrive_time.start_time       1.00      1.00      1.00         1\n",
      "          B-arrive_time.time       0.82      0.78      0.79        40\n",
      "          I-arrive_time.time       0.73      0.81      0.77        27\n",
      " B-arrive_time.time_relative       0.91      0.82      0.86        38\n",
      "                 B-city_name       0.40      0.31      0.35        39\n",
      "                 I-city_name       0.20      0.50      0.29         4\n",
      "                B-class_type       1.00      0.95      0.97        40\n",
      "                I-class_type       1.00      1.00      1.00        33\n",
      "                   B-connect       1.00      0.70      0.82        10\n",
      "             B-cost_relative       1.00      0.99      0.99        93\n",
      "             I-cost_relative       1.00      1.00      1.00        15\n",
      "                  B-day_name       0.00      0.00      0.00         2\n",
      "                B-day_number       0.00      0.00      0.00         2\n",
      "                 B-days_code       0.00      0.00      0.00         2\n",
      " B-depart_date.date_relative       0.92      0.92      0.92        24\n",
      "      B-depart_date.day_name       0.90      0.98      0.94       175\n",
      "    B-depart_date.day_number       0.91      0.95      0.93        64\n",
      "    I-depart_date.day_number       1.00      0.91      0.95        22\n",
      "    B-depart_date.month_name       0.91      0.95      0.93        61\n",
      "B-depart_date.today_relative       0.94      1.00      0.97        16\n",
      "          B-depart_date.year       1.00      1.00      1.00         2\n",
      "      B-depart_time.end_time       0.75      1.00      0.86         3\n",
      "      I-depart_time.end_time       0.67      1.00      0.80         2\n",
      "    B-depart_time.period_mod       0.57      0.80      0.67         5\n",
      " B-depart_time.period_of_day       0.92      0.97      0.94       114\n",
      " I-depart_time.period_of_day       0.50      1.00      0.67         1\n",
      "    B-depart_time.start_time       1.00      1.00      1.00         3\n",
      "    I-depart_time.start_time       1.00      1.00      1.00         2\n",
      "          B-depart_time.time       0.88      0.88      0.88        86\n",
      "          I-depart_time.time       0.91      0.90      0.90        79\n",
      " B-depart_time.time_relative       0.90      0.92      0.91        79\n",
      " I-depart_time.time_relative       0.00      0.00      0.00         2\n",
      "                   B-economy       1.00      1.00      1.00         7\n",
      "                   I-economy       1.00      1.00      1.00         3\n",
      "               B-fare_amount       1.00      0.96      0.98        25\n",
      "               I-fare_amount       1.00      1.00      1.00        24\n",
      "           B-fare_basis_code       0.90      0.95      0.93        20\n",
      "               B-flight_days       1.00      1.00      1.00        10\n",
      "                B-flight_mod       0.97      0.96      0.97        78\n",
      "             B-flight_number       1.00      1.00      1.00        13\n",
      "               B-flight_stop       1.00      0.94      0.97        35\n",
      "               I-flight_stop       1.00      0.75      0.86         4\n",
      "               B-flight_time       1.00      0.95      0.97        19\n",
      "               I-flight_time       1.00      0.50      0.67        12\n",
      "      B-fromloc.airport_code       1.00      0.50      0.67         2\n",
      "      B-fromloc.airport_name       0.93      0.70      0.80        20\n",
      "      I-fromloc.airport_name       0.91      0.72      0.81        29\n",
      "         B-fromloc.city_name       1.00      0.99      0.99       878\n",
      "         I-fromloc.city_name       0.99      0.97      0.98       145\n",
      "        B-fromloc.state_code       1.00      0.93      0.97        15\n",
      "        B-fromloc.state_name       1.00      1.00      1.00         5\n",
      "        I-fromloc.state_name       1.00      1.00      1.00         1\n",
      "                      B-meal       0.85      1.00      0.92        11\n",
      "                 B-meal_code       0.00      0.00      0.00         2\n",
      "                 I-meal_code       0.00      0.00      0.00         2\n",
      "          B-meal_description       1.00      0.89      0.94         9\n",
      "          I-meal_description       0.00      0.00      0.00         1\n",
      "                       B-mod       1.00      0.33      0.50         6\n",
      "                B-month_name       0.00      0.00      0.00         2\n",
      "                        B-or       0.92      1.00      0.96        12\n",
      "             B-period_of_day       0.00      0.00      0.00         1\n",
      "          B-restriction_code       1.00      1.00      1.00         6\n",
      "          I-restriction_code       1.00      1.00      1.00         6\n",
      " B-return_date.date_relative       1.00      0.50      0.67         4\n",
      " I-return_date.date_relative       0.00      0.00      0.00         2\n",
      "    B-return_date.day_number       0.00      0.00      0.00         2\n",
      "    B-return_date.month_name       0.00      0.00      0.00         2\n",
      "B-return_date.today_relative       0.00      0.00      0.00         1\n",
      "I-return_date.today_relative       0.00      0.00      0.00         2\n",
      "    B-return_time.period_mod       0.00      0.00      0.00         1\n",
      " B-return_time.period_of_day       0.00      0.00      0.00         1\n",
      "                B-round_trip       1.00      0.98      0.99        82\n",
      "                I-round_trip       1.00      1.00      1.00        80\n",
      "                B-state_code       1.00      0.67      0.80         3\n",
      "         B-stoploc.city_name       0.62      0.72      0.67        50\n",
      "         I-stoploc.city_name       0.89      0.53      0.67        15\n",
      "                      B-time       0.00      0.00      0.00         1\n",
      "                      I-time       0.00      0.00      0.00         1\n",
      "             B-time_relative       0.00      0.00      0.00         1\n",
      "            B-today_relative       0.00      0.00      0.00         1\n",
      "        B-toloc.airport_code       1.00      0.80      0.89         5\n",
      "        B-toloc.airport_name       0.25      0.67      0.36         3\n",
      "        I-toloc.airport_name       0.15      0.67      0.25         3\n",
      "           B-toloc.city_name       0.97      0.99      0.98       866\n",
      "           I-toloc.city_name       0.95      0.99      0.97       214\n",
      "        B-toloc.country_name       1.00      1.00      1.00         1\n",
      "          B-toloc.state_code       0.91      1.00      0.95        21\n",
      "          B-toloc.state_name       1.00      0.94      0.97        18\n",
      "          I-toloc.state_name       1.00      1.00      1.00         3\n",
      "            B-transport_type       1.00      0.82      0.90        11\n",
      "            I-transport_type       1.00      1.00      1.00         5\n",
      "\n",
      "                 avg / total       0.94      0.93      0.93      4207\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\kshitij jain\\appdata\\local\\programs\\python\\python36-32\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "print(iob_classification_report(y_valid, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
