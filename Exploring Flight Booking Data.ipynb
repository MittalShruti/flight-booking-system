{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flight Booking Data Exploration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overall TODO list:\n",
    "# The general process of named entity recognition is as follows:\n",
    "## - Use high precision rules to extract unambigous entities e.g. \n",
    "## - Use application specific name lists such as airlines, cities etc.\n",
    "## - Finally apply probabalistic models such as CRFs, HMMs etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 588,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "import random, datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 589,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/ipykernel_launcher.py:2: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Show me the cheapest flights from Raipur to Ra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Book a flight to Patna on 17 May 2018 or 18 Ma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Book a flight to Imphal on May 14, 2018 or May...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I want to check availability on SpiceJet on Ma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Any flights between Calicut and Gaya on 15 May...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence\n",
       "0  Show me the cheapest flights from Raipur to Ra...\n",
       "1  Book a flight to Patna on 17 May 2018 or 18 Ma...\n",
       "2  Book a flight to Imphal on May 14, 2018 or May...\n",
       "3  I want to check availability on SpiceJet on Ma...\n",
       "4  Any flights between Calicut and Gaya on 15 May..."
      ]
     },
     "execution_count": 589,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# flights dataset\n",
    "df = pd.read_csv('flightdata.txt', sep='/n', header=None)\n",
    "df.columns = ['sentence']\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 590,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4413    Any flights between Nagpur and Pune on May 18,...\n",
       "3516    Book a flight from Bhubaneswar on 17 May 2018 ...\n",
       "954     Show me the cheapest flights from Pune to Raip...\n",
       "548     Show me the cheapest flights from Bengaluru to...\n",
       "771     Book a flight from Imphal on 27 May after 17 h...\n",
       "3549    I want to check availability on Air India on M...\n",
       "1616    Any flights between Nagpur and Shillong on 04 ...\n",
       "49      Book a flight to Aizawl on 04 June or 05 June ...\n",
       "2463    Book a flight from Patna on May 05, 2018 after...\n",
       "659     Can you please show me flights from Visakhapat...\n",
       "2179    Book a flight to Guwahati on May 20, 2018 or M...\n",
       "1218    Show me the cheapest flights from Patna to Guw...\n",
       "4613    Show me the cheapest flights from Patna to Shi...\n",
       "645     Book a flight to Gaya on May 17, 2018 or May 1...\n",
       "2419    Book a flight to Diu on May 27, 2018 or May 28...\n",
       "3184    Please help me in booking flight from Guwahati...\n",
       "3512    Show me the cheapest flights from New Delhi to...\n",
       "4264    I want to check availability on SpiceJet on Ma...\n",
       "3320    Book a flight to Bengaluru on 31 May 2018 or 0...\n",
       "4661    Any flights between Pondicherry and Ranchi on ...\n",
       "4551    Book a flight to Cochin on 28 May 2018 or 29 M...\n",
       "243     Can you please show me flights from Jammu to G...\n",
       "964     Show me the cheapest flights from Patna to New...\n",
       "3720    Please help me in booking flight from Diu to V...\n",
       "4624    Book a flight to Srinagar on June 04, 2018 or ...\n",
       "2113    Please help me in booking flight from Calicut ...\n",
       "581     Please show me the cheapest flight from Imphal...\n",
       "2630    Book a flight from New Delhi on 27 May after 1...\n",
       "4186    Please help me in booking flight from Bengalur...\n",
       "1799    Show me the cheapest flights from Srinagar to ...\n",
       "                              ...                        \n",
       "507     Can you please show me flights from Ranchi to ...\n",
       "2354    I want to book flight between Calicut and Indo...\n",
       "106     Any flights between Srinagar and Bengaluru on ...\n",
       "1675    Book a flight to Ranchi on 06 May 2018 or 07 M...\n",
       "4153    Book a flight to Pondicherry on 27 April or 28...\n",
       "1030    I want to book flight between Ranchi and Gaya ...\n",
       "1378    Book a flight from Imphal on 28 May 2018 after...\n",
       "2378    I want to book flight between Raipur and Patna...\n",
       "1512    Book a flight from Imphal on 19 May 2018 after...\n",
       "2043    Show me the cheapest flights from Ahmedabad to...\n",
       "784     Please help me in booking flight from Aizawl t...\n",
       "4442    Can you please show me flights from Nagpur to ...\n",
       "3162    Book a flight from Trivandrum on April 28, 201...\n",
       "650     Book a flight to Raipur on May 27, 2018 or May...\n",
       "2118    I want to check availability on Vistara on 30 ...\n",
       "1661    I want to check availability on Air India on 0...\n",
       "1165    Book a flight to Calicut on 30 May 2018 or 31 ...\n",
       "4407    I want to check availability on Vistara on 11 ...\n",
       "3712    Book a flight to Indore on 01 May or 02 May be...\n",
       "2799    Book a flight to Gaya on May 28, 2018 or May 2...\n",
       "1592    Book a flight to Bhopal on May 20, 2018 or May...\n",
       "3742    I want to check availability on SpiceJet on Ma...\n",
       "2954    Book a flight from Calicut on May 19, 2018 aft...\n",
       "3126    Please help me in booking flight from Raipur t...\n",
       "3473    Show me the cheapest flights from Patna to Vis...\n",
       "3814    Book a flight to Diu on April 29, 2018 or Apri...\n",
       "2456    I want to book flight between Indore and Cochi...\n",
       "3547    I want to check availability on Vistara on May...\n",
       "3884    Show me the cheapest flights from Jammu to Pun...\n",
       "11      I want to check availability on AirAsia India ...\n",
       "Name: sentence, Length: 236, dtype: object"
      ]
     },
     "execution_count": 590,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sample sentences\n",
    "df.sample(frac=0.05)['sentence']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Broad Approach\n",
    "\n",
    "The objective is to write a program which takes in a sentence as input and converts it into a semi-stuctured/structured format (such as a dict) containing fields such as from_destination, to_destination, date_of_travel, airline_type etc.\n",
    "\n",
    "We'll first do two basic preprocessing tasks for each input sentence - tokenization and POS tagging. After preprocessing, we'll identify named entities such as locations (cities), dates, amount/momney, flight companies etc. Once we identify named entities, we can parse the sentence and try extracting relations between entities (from X to Y on date Z etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 591,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing function\n",
    "def preprocess(sentence):\n",
    "    words = nltk.word_tokenize(sentence)\n",
    "    tagged_words = nltk.pos_tag(words)\n",
    "    return tagged_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 593,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Show', 'VB'),\n",
       " ('me', 'PRP'),\n",
       " ('the', 'DT'),\n",
       " ('cheapest', 'JJS'),\n",
       " ('flights', 'NNS'),\n",
       " ('from', 'IN'),\n",
       " ('Pune', 'NNP'),\n",
       " ('to', 'TO'),\n",
       " ('Bengaluru', 'VB'),\n",
       " ('on', 'IN'),\n",
       " ('June', 'NNP'),\n",
       " ('03', 'CD'),\n",
       " (',', ','),\n",
       " ('2018', 'CD'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 593,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# preprocessing some sample sentences\n",
    "i = random.randrange(len(df.index))\n",
    "tagged_sent = preprocess(df.loc[67, 'sentence'])\n",
    "tagged_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 530,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example sentence tagging city_2 as VB\n",
    "# i = 67"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 594,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Show/VB me/PRP the/DT cheapest/JJS flights/NNS from/IN Pune/NNP to/TO Bengaluru/VB on/IN June/NNP 03/CD ,/, 2018/CD ./.'"
      ]
     },
     "execution_count": 594,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# printing in string format\n",
    "' '.join(['{0}/{1}'.format(s[0], s[1]) for s in tagged_sent])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identifying Dates, Cities, Price Range etc.\n",
    "\n",
    "Next, we need to identify important categories of information, such as the date and time of travel, to and from destination cities, price constraints (if any), airline provider (if any), etc. A fully specified dictionary for the following query will look something like this:\n",
    "\n",
    "**Example query:** <br>*I want to book a flight from patna to bangalore between 21 May and 23 May after 5 PM on either Air India or Jet Airways*.\n",
    "\n",
    "Note that we are assuming that the current year is 2018 and that *after 5 PM* refers to the departure time from city_1 (not arrival time at city_2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 532,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample fully specified dict\n",
    "sample_dict = {'from_location': 'patna', \n",
    "               'to_location': 'bengaluru', \n",
    "               'provider': ['airindia, jet'],\n",
    "               'depart_day': ['21-05-2018', '23-05-2018'],\n",
    "               'skip_day': None,\n",
    "               'depart_time_after': 1700,\n",
    "               'depart_time_before': None\n",
    "              }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regular Expressions Based Approach\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 613,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "\n",
    "# - dictionary\n",
    "#      - dates: ['today', 'tomorrow']\n",
    "#      - cities: ['patna', 'bengaluru']\n",
    "#      - airlines: [...]\n",
    "#      - time: [3 pm, noon]\n",
    "#      - money: [INR xyz]\n",
    "    \n",
    "# - search regexes, e.g. from source_x to dest_y, on date xyz etc.\n",
    "# - INR/Rs followed by CD is money"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chunking Based Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chunking is used to identify meaningful groups (or chunks) of a sentence and is commonly used for entity recognition. \n",
    "\n",
    "In our example, we would want to identify **date chunks** (e.g. on Feb 21, 2018), or **noun (source/destination city) chunks** (e.g. from patna to mumbai) etc. \n",
    "\n",
    "It is sometimes also called shallow parsing, since we are only interested in identifying noun or verb phrases, but not necessarily in knowing that the noun phrase is the subject of the sentence.\n",
    "\n",
    "The following example describes chunking briefly.\n",
    "\n",
    "- <a href=\"https://stackoverflow.com/questions/1598940/in-natural-language-processing-what-is-the-purpose-of-chunking?utm_medium=organic&utm_source=google_rich_qa&utm_campaign=google_rich_qa\">A brief description of chunking (SO answer)</a>\n",
    "\n",
    "The NLTK book explain chunking in detail (highly recommended read before moving on):\n",
    "- <a href=\"https://www.safaribooksonline.com/library/view/natural-language-processing/9780596803346/ch07s02.html\">NLTK book: Information Extraction using Chunking</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting Dates\n",
    "\n",
    "Let's first look at extracting dates from the queries. We'll use 'chunking' to do that. Chunks are basically (regex) patterns of the POS tags. For e.g. look at the following few sentences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 595,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Show/VB me/PRP the/DT cheapest/JJS flights/NNS from/IN Imphal/NNP to/TO Patna/VB on/IN 16/CD May/NNP ./.'"
      ]
     },
     "execution_count": 595,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# date example-1 (the 1621st sentence)\n",
    "tagged_sent = preprocess(df.loc[1621, 'sentence'])\n",
    "' '.join(['{0}/{1}'.format(s[0], s[1]) for s in tagged_sent])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The date chunk in the above query is ```on/IN 16/CD May/NNP ./.'```. We can extract the date chunk by defining a regex as follows: *an optional preposition IN followed by a cardinal CD followed by a proper noun NNP*.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 596,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  Show/VB\n",
      "  me/PRP\n",
      "  the/DT\n",
      "  cheapest/JJS\n",
      "  flights/NNS\n",
      "  from/IN\n",
      "  Imphal/NNP\n",
      "  to/TO\n",
      "  Patna/VB\n",
      "  (date_chunk on/IN 16/CD May/NNP)\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "# defining a simple date chunk\n",
    "# try extracting the dates using a chunk grammar\n",
    "grammar = 'date_chunk: {<IN>?<CD><NNP>?}'\n",
    "cp = nltk.RegexpParser(grammar)\n",
    "\n",
    "# choose a sentence s\n",
    "s = df.loc[1621, 'sentence']\n",
    "chunks = cp.parse(preprocess(s))\n",
    "print(chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This seems to work fine on the given query, because it had only one date chunk. It will not work on the queries below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 597,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Please/VB help/VB me/PRP in/IN booking/VBG flight/NN from/IN Cochin/NNP to/TO Bhopal/NNP on/IN either/CC AirAsia/NNP India/NNP or/CC SpiceJet/NNP between/IN 01/CD June/NNP 2018/CD and/CC 25/CD June/NNP 2018/CD ./.'"
      ]
     },
     "execution_count": 597,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# date example-2: 669th query\n",
    "tagged_sent = preprocess(df.loc[669, 'sentence'])\n",
    "' '.join(['{0}/{1}'.format(s[0], s[1]) for s in tagged_sent])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the sentence above, the date chunk we're interested in is ```between/IN 01/CD June/NNP 2018/CD and/CC 25/CD June/NNP 2018/CD ./.'``` Another similar example is given below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 598,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Book/VB a/DT flight/NN to/TO Calicut/VB on/IN 15/CD May/NNP or/CC 16/CD May/NNP before/IN 17/CD hours/NNS ./.'"
      ]
     },
     "execution_count": 598,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# date example-3\n",
    "tagged_sent = preprocess(df.loc[3334, 'sentence'])\n",
    "' '.join(['{0}/{1}'.format(s[0], s[1]) for s in tagged_sent])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this sentence, there are two chunks of interest   ```on/IN 15/CD May/NNP``` or/CC ```16/CD May/NNP```. \n",
    "\n",
    "Also, since dates can be specified as 28 May, 2018 and 28 May 2018, an optional comma should be included (note that the POS tag of a comma is comma itself). Also, the order of CD and NNP can be reversed (e.g. May 20, 2018 or 20 May, 2018).\n",
    "\n",
    "The grammar for including these cases can defined as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 599,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  I/PRP\n",
      "  want/VBP\n",
      "  to/TO\n",
      "  check/VB\n",
      "  availability/NN\n",
      "  on/IN\n",
      "  IndiGo/NNP\n",
      "  (date_chunk on/IN May/NNP 28/CD ,/, 2018/CD)\n",
      "  for/IN\n",
      "  flights/NNS\n",
      "  from/IN\n",
      "  Trivandrum/NNP\n",
      "  to/TO\n",
      "  Aizawl/NNP\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "# modifying the grammar further\n",
    "# put an optional comma\n",
    "# include May 20, 2018 and 20 May, 2018\n",
    "grammar = r'''\n",
    "date_chunk: {<IN>?<CD><NNP><,>?<CD>?}   # e.g. on 28 May, 2018\n",
    "            {<IN>?<NNP><CD><,>?<CD>?}   # e.g. on May 28, 2018            \n",
    "            '''\n",
    "\n",
    "cp = nltk.RegexpParser(grammar)\n",
    "\n",
    "# Note that May 28, 2018 is also correcty parsed now\n",
    "s = df.loc[1846, 'sentence']\n",
    "chunks = cp.parse(preprocess(s))\n",
    "print(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 600,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  Show/VB\n",
      "  me/PRP\n",
      "  the/DT\n",
      "  cheapest/JJS\n",
      "  flights/NNS\n",
      "  from/IN\n",
      "  Raipur/NNP\n",
      "  to/TO\n",
      "  Visakhapatnam/NNP\n",
      "  (date_chunk on/IN 18/CD May/NNP 2018/CD)\n",
      "  or/CC\n",
      "  (date_chunk 19/CD May/NNP 2018/CD)\n",
      "  or/CC\n",
      "  (date_chunk 20/CD May/NNP 2018/CD)\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "# testing the grammar on some random sentences\n",
    "i = random.randrange(len(df.index))\n",
    "s = df.loc[i, 'sentence']\n",
    "chunks = cp.parse(preprocess(s))\n",
    "print(chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The date grammar seems to be working fine in most cases, apart from those which contain the phrases *today, tomorrow* etc. \n",
    "\n",
    "Also, there are some **false positives**, such as this (```up to INR xxx``` is chunked as a date):\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 601,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  Any/DT\n",
      "  flights/NNS\n",
      "  between/IN\n",
      "  Bhopal/NNP\n",
      "  and/CC\n",
      "  Imphal/NNP\n",
      "  (date_chunk on/IN April/NNP 29/CD ,/, 2018/CD)\n",
      "  up/IN\n",
      "  to/TO\n",
      "  (date_chunk INR/NNP 4000/CD)\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "# false positive example\n",
    "i = 1216\n",
    "s = df.loc[i, 'sentence']\n",
    "chunks = cp.parse(preprocess(s))\n",
    "print(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ideas:\n",
    "## - NNP but not INR\n",
    "## - NNP should not be followed by TO\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the query above, the phrase INR 4000 matches with the expression ```{<IN>?<NNP><CD><,>?<CD>?}```, so we need to find a way to avoid parsing INR 4000 as a date chunk.\n",
    "\n",
    "One way to do that could be based on the observation that INR 4000 is usually grouped by a ```TO``` or a ```RB``` (adverb) tag, e.g. *up to INR 3000*, while a date chunk contains a IN tag (but not TO), e.g. *on May 28, 2018'.\n",
    "\n",
    "So we can specify a new chunk grammar to match price phrases of the form *up to INR 3000* **before the date chunk**. The parser will classify the phrase according to the price chunk first, and then try the date chunk.\n",
    "\n",
    "**Tip**: The ```trace = True``` argument lets us see the order in which the components of the grammar are parsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 602,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Input:\n",
      " <DT>  <NNS>  <IN>  <NNP>  <CC>  <NNP>  <IN>  <CD>  <NNP>  <RB>  <TO>  <NNP>  <CD>  <.> \n",
      "# price - up to INR 3000:\n",
      " <DT>  <NNS>  <IN>  <NNP>  <CC>  <NNP>  <IN>  <CD>  <NNP> {<RB>  <TO>  <NNP>  <CD>} <.> \n",
      "# price - up to 3000 INR:\n",
      " <DT>  <NNS>  <IN>  <NNP>  <CC>  <NNP>  <IN>  <CD>  <NNP> {<RB>  <TO>  <NNP>  <CD>} <.> \n",
      "# Input:\n",
      " <DT>  <NNS>  <IN>  <NNP>  <CC>  <NNP>  <IN>  <CD>  <NNP>  <price_chunk>  <.> \n",
      "# date - e.g. on 28 May, 2018:\n",
      " <DT>  <NNS>  <IN>  <NNP>  <CC>  <NNP> {<IN>  <CD>  <NNP>} <price_chunk>  <.> \n",
      "# date - e.g. on May 28, 2018:\n",
      " <DT>  <NNS>  <IN>  <NNP>  <CC>  <NNP> {<IN>  <CD>  <NNP>} <price_chunk>  <.> \n",
      "(S\n",
      "  Any/DT\n",
      "  flights/NNS\n",
      "  between/IN\n",
      "  Dimapur/NNP\n",
      "  and/CC\n",
      "  Mumbai/NNP\n",
      "  (date_chunk on/IN 08/CD May/NNP)\n",
      "  (price_chunk up/RB to/TO INR/NNP 7500/CD)\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "# adding a price chunk before the date chunk\n",
    "# In 'up to INR 3k', up can either be tagged as RB or IN\n",
    "# put an optional comma\n",
    "# include May 20, 2018 and 20 May, 2018\n",
    "grammar = r'''\n",
    "price_chunk: {<RB>?<IN>?<TO><NNP><CD>}  # price - up to INR 3000\n",
    "             {<RB>?<IN>?<TO><CD><NNP>}  # price - up to 3000 INR\n",
    "\n",
    "date_chunk: {<IN>?<CD><NNP><,>?<CD>?}   # date - e.g. on 28 May, 2018\n",
    "            {<IN>?<NNP><CD><,>?<CD>?}   # date - e.g. on May 28, 2018            \n",
    "            '''\n",
    "\n",
    "cp = nltk.RegexpParser(grammar)\n",
    "\n",
    "\n",
    "# trace = 1 argument lets us see the order in which the components of \n",
    "# the grammar are parsed\n",
    "s = df.loc[2005, 'sentence']\n",
    "chunks = cp.parse(preprocess(s), trace=True)\n",
    "print(chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The trace above shows that the price chunk is matched first (and matches ```{<IN>  <TO>  <NNP>  <CD>}``` ), and then the date chunk is matched."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 606,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  Can/MD\n",
      "  you/PRP\n",
      "  please/VB\n",
      "  show/VB\n",
      "  me/PRP\n",
      "  flights/NNS\n",
      "  from/IN\n",
      "  New/NNP\n",
      "  Delhi/NNP\n",
      "  to/TO\n",
      "  Pondicherry/NNP\n",
      "  tomorrow/NN\n",
      "  ?/.\n",
      "  I/PRP\n",
      "  want/VBP\n",
      "  to/TO\n",
      "  avoid/VB\n",
      "  early/JJ\n",
      "  morning/NN\n",
      "  flights/NNS\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "# testing the new grammar on some random sentences\n",
    "i = random.randrange(len(df.index))\n",
    "s = df.loc[i, 'sentence']\n",
    "chunks = cp.parse(preprocess(s))\n",
    "print(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 542,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Date Chunks TODO list\n",
    "\n",
    "# today: 242\n",
    "# tomorrow: 2208\n",
    "# false positive INR 3500: i=348, 1216\n",
    "\n",
    "# Ashish:\n",
    "## today/tomorrow: \n",
    "## early morning: 0400-1000\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting Source and Destination Cities\n",
    "\n",
    "Let's now try extracting source and destinations cities. The most typical phrase is ```from city_1 to city_2```, though city_1 and city_2 both are optional (some queries specify only one of source or destination cities). \n",
    "\n",
    "\n",
    "Note that we need to **specify an optional NNP** regex ```<NNP><NNP>?``` to capture names\n",
    "such as ```New/NNP Delhi/NNP```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 607,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modifying the grammar further\n",
    "grammar = r'''\n",
    "price_chunk: {<RB>?<IN>?<TO><NNP><CD>}  # price - up to INR 3000\n",
    "             {<RB>?<IN>?<TO><CD><NNP>}  # price - up to 3000 INR\n",
    "\n",
    "date_chunk: {<IN>?<CD><NNP><,>?<CD>?}   # e.g. on 28 May, 2018\n",
    "            {<IN>?<NNP><CD><,>?<CD>?}   # e.g. on May 28, 2018  \n",
    "\n",
    "cities: {<IN><NNP><NNP>?<TO><NNP><NNP>?} # from city_1 to city_2\n",
    "        {<IN><NNP><NNP>?<CC><NNP><NNP>?} # between city_1 and city_2\n",
    "        {<IN><NNP><NNP>?}                # from city_1\n",
    "        {<TO><NNP><NNP>?}                # to city_2\n",
    "        \n",
    "            \n",
    "            '''\n",
    "cp = nltk.RegexpParser(grammar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 612,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  I/PRP\n",
      "  want/VBP\n",
      "  to/TO\n",
      "  check/VB\n",
      "  availability/NN\n",
      "  (cities on/IN Vistara/NNP)\n",
      "  (date_chunk on/IN 02/CD May/NNP)\n",
      "  for/IN\n",
      "  flights/NNS\n",
      "  (cities from/IN Ahmedabad/NNP to/TO Cochin/NNP)\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "# parse randomly chosen sentences\n",
    "i = random.randrange(len(df.index))\n",
    "s = df.loc[1992, 'sentence']\n",
    "chunks = cp.parse(preprocess(s))\n",
    "print(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 547,
   "metadata": {},
   "outputs": [],
   "source": [
    "# City chunks TODO  \n",
    "\n",
    "# i = 1992 - false positive - (on vistara, on goair etc. are false positives)\n",
    "# i = 4353  (to city_2 tagged as VB)\n",
    "\n",
    "# ideas:\n",
    "## - for false positives such as on goair: we can just lookup a list of providers \n",
    "##   to check if the phrase contains a name such as goair, airindia etc.\n",
    "airlines = ['vistara', 'air india', 'jet airways', \n",
    "            'indigo', 'spice jet', 'go air', 'air asia',\n",
    "           ' air asia india', 'air deccan']\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
